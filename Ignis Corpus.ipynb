{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignis: Text Pre-processing\n",
    "=========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T02:40:22.883815Z",
     "start_time": "2020-09-10T02:40:20.687036Z"
    }
   },
   "outputs": [],
   "source": [
    "# General library imports\n",
    "import collections\n",
    "import glob\n",
    "import pathlib\n",
    "import re\n",
    "import threading\n",
    "import time\n",
    "\n",
    "import gensim\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import ignis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T02:40:22.896783Z",
     "start_time": "2020-09-10T02:40:22.885784Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "   div.cell > div.output_wrapper > div.output.output_scroll {\n",
       "     height: auto;\n",
       "   }\n",
       "   .jupyter-widgets-output-area .output_scroll {\n",
       "        height: unset;\n",
       "        border-radius: unset;\n",
       "        -webkit-box-shadow: unset;\n",
       "        box-shadow: unset;\n",
       "    }\n",
       "    .jupyter-widgets-output-area, div.output_stdout, div.output_result  {\n",
       "        height: auto;\n",
       "        max-height: 60em;\n",
       "        overflow-y: auto;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Jupyter notebook setup\n",
    "import ipywidgets as widgets\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# Custom styling:\n",
    "# - Prevent vertical scrollbars in output subareas\n",
    "style = \"\"\"\n",
    "<style>\n",
    "   div.cell > div.output_wrapper > div.output.output_scroll {\n",
    "     height: auto;\n",
    "   }\n",
    "   .jupyter-widgets-output-area .output_scroll {\n",
    "        height: unset;\n",
    "        border-radius: unset;\n",
    "        -webkit-box-shadow: unset;\n",
    "        box-shadow: unset;\n",
    "    }\n",
    "    .jupyter-widgets-output-area, div.output_stdout, div.output_result  {\n",
    "        height: auto;\n",
    "        max-height: 60em;\n",
    "        overflow-y: auto;\n",
    "    }\n",
    "</style>\n",
    "\"\"\"\n",
    "display(HTML(style))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data ingestion\n",
    "--------------------\n",
    "\n",
    "We will track the contents and filename of each document, then tokenise them all and feed them into an `ignis.Corpus` that will be saved.\n",
    "\n",
    "We should, by all accounts, actually be preparing a separate text cleaning function and running the raw text through it immediately, but this way we can see the effects of each step of the data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T02:40:22.924796Z",
     "start_time": "2020-09-10T02:40:22.899785Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_files = glob.glob(\"./data/bbc/*/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T02:40:22.931818Z",
     "start_time": "2020-09-10T02:40:22.926804Z"
    }
   },
   "outputs": [],
   "source": [
    "RawDocument = collections.namedtuple(\"RawDocument\", \"metadata, tokens, display_str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T02:40:23.261793Z",
     "start_time": "2020-09-10T02:40:22.933784Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "646f307ebf0d453691cc76bcefe3a6eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2225.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "raw_docs = []\n",
    "for file in tqdm(raw_files):\n",
    "    filename = pathlib.Path(file).as_posix()\n",
    "\n",
    "    metadata = {\"filename\": filename}\n",
    "\n",
    "    # Basic HTML conversion (we could just use the plain text as well, BeautifulSoup can parse it)\n",
    "    with open(file) as f:\n",
    "        tokens = f.read()\n",
    "        lines = [line for line in tokens.split(\"\\n\") if line != \"\"]\n",
    "\n",
    "        # Assume first line is the title\n",
    "        title = f\"<strong>{lines[0]}</strong>\"\n",
    "        paras = [f\"<p>{line}</p>\" for line in lines[1:]]\n",
    "        body = \"\\n\".join(paras)\n",
    "        display_str = f\"<html><body>{title}{body}</body></html>\"\n",
    "\n",
    "    raw_doc = RawDocument(metadata, tokens, display_str)\n",
    "\n",
    "    raw_docs.append(raw_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T02:40:23.287815Z",
     "start_time": "2020-09-10T02:40:23.262785Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c03e34d1989241c5bcf5d76436fd0c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='doc_id', max=2224), Output()), _dom_classes=('widget-int…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.show_raw_doc(doc_id=0)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def show_raw_doc(doc_id=0):\n",
    "    print(raw_docs[doc_id].metadata)\n",
    "    print()\n",
    "    print(raw_docs[doc_id].tokens)\n",
    "\n",
    "\n",
    "widgets.interact(show_raw_doc, doc_id=(0, len(raw_docs) - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text pre-processing and tokenisation\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-tokenisation cleaning\n",
    "- URLs/Emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T02:40:23.342843Z",
     "start_time": "2020-09-10T02:40:23.289797Z"
    }
   },
   "outputs": [],
   "source": [
    "URL_REGEX = re.compile(\n",
    "    # protocol identifier\n",
    "    r\"(?:(?:(?:https?|ftp):)?//)\"\n",
    "    # user:pass authentication\n",
    "    r\"(?:\\S+(?::\\S*)?@)?\" r\"(?:\"\n",
    "    # IP address exclusion\n",
    "    # private & local networks\n",
    "    r\"(?!(?:10|127)(?:\\.\\d{1,3}){3})\"\n",
    "    r\"(?!(?:169\\.254|192\\.168)(?:\\.\\d{1,3}){2})\"\n",
    "    r\"(?!172\\.(?:1[6-9]|2\\d|3[0-1])(?:\\.\\d{1,3}){2})\"\n",
    "    # IP address dotted notation octets\n",
    "    # excludes loopback network 0.0.0.0\n",
    "    # excludes reserved space >= 224.0.0.0\n",
    "    # excludes network & broadcast addresses\n",
    "    # (first & last IP address of each class)\n",
    "    r\"(?:[1-9]\\d?|1\\d\\d|2[01]\\d|22[0-3])\"\n",
    "    r\"(?:\\.(?:1?\\d{1,2}|2[0-4]\\d|25[0-5])){2}\"\n",
    "    r\"(?:\\.(?:[1-9]\\d?|1\\d\\d|2[0-4]\\d|25[0-4]))\"\n",
    "    r\"|\"\n",
    "    # host & domain names, may end with dot\n",
    "    # can be replaced by a shortest alternative\n",
    "    # r\"(?![-_])(?:[-\\w\\u00a1-\\uffff]{0,63}[^-_]\\.)+\"\n",
    "    # r\"(?:(?:[a-z\\u00a1-\\uffff0-9]-?)*[a-z\\u00a1-\\uffff0-9]+)\"\n",
    "    # # domain name\n",
    "    # r\"(?:\\.(?:[a-z\\u00a1-\\uffff0-9]-?)*[a-z\\u00a1-\\uffff0-9]+)*\"\n",
    "    r\"(?:\"\n",
    "    r\"(?:\"\n",
    "    r\"[a-z0-9\\u00a1-\\uffff]\"\n",
    "    r\"[a-z0-9\\u00a1-\\uffff_-]{0,62}\"\n",
    "    r\")?\"\n",
    "    r\"[a-z0-9\\u00a1-\\uffff]\\.\"\n",
    "    r\")+\"\n",
    "    # TLD identifier name, may end with dot\n",
    "    r\"(?:[a-z\\u00a1-\\uffff]{2,}\\.?)\" r\")\"\n",
    "    # port number (optional)\n",
    "    r\"(?::\\d{2,5})?\"\n",
    "    # resource path (optional)\n",
    "    r\"(?:[/?#]\\S*)?\",\n",
    "    re.IGNORECASE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T02:40:23.353798Z",
     "start_time": "2020-09-10T02:40:23.346784Z"
    }
   },
   "outputs": [],
   "source": [
    "pre_regexes = [\n",
    "    # Rough email regex\n",
    "    re.compile(r\"\\b[a-z0-9._%+-]+@[a-z0-9.-]+[.][a-z]{2,}\\b\", re.IGNORECASE),\n",
    "    # URL regex\n",
    "    URL_REGEX,\n",
    "    # Rougher URL regex\n",
    "    re.compile(r\"\\bwww[.][a-z0-9.-]+[.][a-z]{2,}\\b\", re.IGNORECASE),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T02:40:24.090918Z",
     "start_time": "2020-09-10T02:40:23.355790Z"
    }
   },
   "outputs": [],
   "source": [
    "def pre_regex(doc):\n",
    "    for regex in pre_regexes:\n",
    "        doc = regex.sub(\"\", doc)\n",
    "    return doc\n",
    "\n",
    "\n",
    "pre_clean_docs = []\n",
    "for doc in raw_docs:\n",
    "    pre_clean_docs.append(doc._replace(tokens=pre_regex(doc.tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive tokenisation (by whitespace)\n",
    "- Case folding\n",
    "- Strip leading/trailing non-informative punctuation from tokens\n",
    "- Remove single apostrophes\n",
    "- Remove single brackets within words\n",
    "  - For dealing with cases like \"the recipient(s)\" -- Which will get tokenised to \"the recipient(s\" otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T02:40:24.098487Z",
     "start_time": "2020-09-10T02:40:24.091789Z"
    }
   },
   "outputs": [],
   "source": [
    "strip_punctuation = \"'\\\"()[]<>?!,.:;/|_\"\n",
    "bracket_pairs = [\n",
    "    [\"(\", \")\"],\n",
    "    [\"[\", \"]\"],\n",
    "]\n",
    "\n",
    "\n",
    "def naive_tokenise(doc):\n",
    "    \"\"\"\n",
    "    Naively tokenises a document.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The document as a string of space-separated tokens\n",
    "    \"\"\"\n",
    "    new_tokens = []\n",
    "\n",
    "    tokens = doc.split()\n",
    "    for token in tokens:\n",
    "        token = token.casefold()\n",
    "        token = token.strip(strip_punctuation)\n",
    "        token = token.replace(\"'\", \"\")\n",
    "\n",
    "        for bracket_pair in bracket_pairs:\n",
    "            if bracket_pair[0] in token and bracket_pair[1] not in token:\n",
    "                token = token.replace(bracket_pair[0], \"\")\n",
    "            if bracket_pair[1] in token and bracket_pair[0] not in token:\n",
    "                token = token.replace(bracket_pair[1], \"\")\n",
    "\n",
    "        if token != \"\":\n",
    "            new_tokens.append(token)\n",
    "\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T02:40:24.680814Z",
     "start_time": "2020-09-10T02:40:24.101785Z"
    }
   },
   "outputs": [],
   "source": [
    "naive_docs = []\n",
    "for doc in pre_clean_docs:\n",
    "    naive_docs.append(doc._replace(tokens=naive_tokenise(doc.tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T02:40:24.717784Z",
     "start_time": "2020-09-10T02:40:24.681785Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dfaa0491a904ce7a9c42a2a01558397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='doc_id', max=2224), Output()), _dom_classes=('widget-int…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.show_naive_doc(doc_id=0)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def show_naive_doc(doc_id=0):\n",
    "    print(naive_docs[doc_id].metadata)\n",
    "    print()\n",
    "    print(\" \".join(naive_docs[doc_id].tokens))\n",
    "\n",
    "\n",
    "widgets.interact(show_naive_doc, doc_id=(0, len(naive_docs) - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automated n-gram detection\n",
    "\n",
    "Chunk into significant bigrams based on collocation frequency\n",
    "\n",
    "(N.B.: Gensim implies that the input to the Phraser should be a list of single sentences, but we will feed it a list of documents instead.)\n",
    "\n",
    "- Min count: How many documents the n-grams need to appear in\n",
    "\n",
    "- Scoring: \"default\" or \"npmi\"\n",
    "\n",
    "- Threshold: Intuitively, higher threshold means fewer phrases.\n",
    "  - With the default scorer, this is greater than or equal to 0; with the NPMI scorer, this is in the range -1 to 1.\n",
    "\n",
    "- Common terms: These terms will be ignored if they come between normal words.\n",
    "  - E.g., if `common_terms` includes the word \"of\", then when the phraser sees \"Wheel of Fortune\" it actually evaluates _\"Wheel Fortune\"_ as an n-gram, putting \"of\" back in only at the output level to give _wheel_of_fortune_.\n",
    "  - With the `common_terms` option set properly,  there do not seem to be many significant trigrams and above left to pick up -- We stick with bigrams to save on runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T02:40:24.725783Z",
     "start_time": "2020-09-10T02:40:24.720796Z"
    }
   },
   "outputs": [],
   "source": [
    "min_count = 5\n",
    "scoring = \"npmi\"\n",
    "# We want a relatively high threshold so that we don't start littering spurious n-grams all over our corpus, diluting our results.\n",
    "# E.g., we want \"Lord_of_the_Rings\", but not \"slightly_better_than_analysts\"\n",
    "threshold = 0.7\n",
    "common_terms = [\"a\", \"an\", \"the\", \"of\", \"on\", \"in\", \"at\"]\n",
    "\n",
    "# Up to 4-grams, not counting `common_terms`\n",
    "phraser_iterations = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T02:40:40.220446Z",
     "start_time": "2020-09-10T02:40:24.728788Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1...\n",
      "Gensim Phraser initialised. 4.848s\n",
      "Improved Phraser initialised. 0.035s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a47323d865141e7a7a91266c279c95c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2225.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 2...\n",
      "Gensim Phraser initialised. 4.930s\n",
      "Improved Phraser initialised. 0.007s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14b0d785f0324e4383c21b916d50f1af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2225.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 3...\n",
      "Gensim Phraser initialised. 4.579s\n",
      "Improved Phraser initialised. 0.001s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61d1a46426e843db8d91764bf7343b6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2225.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for_phrasing = [doc.tokens for doc in naive_docs]\n",
    "\n",
    "for i in range(phraser_iterations):\n",
    "    print(f\"Iteration {i + 1}...\")\n",
    "    \n",
    "    phraser = ignis.util.ImprovedPhraser(\n",
    "        for_phrasing,\n",
    "        min_count=min_count,\n",
    "        threshold=threshold,\n",
    "        scoring=scoring,\n",
    "        common_terms=common_terms,\n",
    "        drop_non_alpha=True,\n",
    "        verbose=True,\n",
    "    )\n",
    "    for_phrasing = phraser.find_ngrams(for_phrasing, verbose=True)\n",
    "\n",
    "# `for_phrasing` contains the post-phrasing tokens for each document;\n",
    "# We need to recombine them with each document's metadata etc.\n",
    "phrased_docs = []\n",
    "for index, doc in enumerate(for_phrasing):\n",
    "    phrased_docs.append(naive_docs[index]._replace(tokens=doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T02:40:40.256414Z",
     "start_time": "2020-09-10T02:40:40.222414Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bfd0dc2d395441fbd34948b020c20ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='doc_id', max=2224), Output()), _dom_classes=('widget-int…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.show_phrased_doc(doc_id=0)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def show_phrased_doc(doc_id=0):\n",
    "    print(phrased_docs[doc_id].metadata)\n",
    "    print()\n",
    "    print(\"/\".join(phrased_docs[doc_id].tokens))\n",
    "\n",
    "\n",
    "widgets.interact(show_phrased_doc, doc_id=(0, len(phrased_docs) - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T02:40:40.482414Z",
     "start_time": "2020-09-10T02:40:40.260423Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lord of the rings\n",
      "reuters news agency\n",
      "founder mikhail khodorkovsky\n",
      "wall street journal\n",
      "gross domestic product\n",
      "sports utility vehicles\n",
      "radio 4s today programme\n",
      "standard & poors\n",
      "chancellor gerhard schroeder\n",
      "judge letitia clark\n",
      "gas monopoly gazprom\n",
      "told bbc news\n",
      "sir digby jones\n",
      "president george w bush\n",
      "house of lords\n",
      "george w bushs\n",
      "international monetary fund\n",
      "tens of thousands\n",
      "monetary fund imf\n",
      "governor mervyn king\n",
      "chambers of commerce\n",
      "shadow chancellor oliver letwin\n",
      "billions of dollars\n",
      "chapter 11 bankruptcy\n",
      "faces stiff competition\n",
      "gross domestic product gdp\n",
      "sir alex ferguson\n",
      "tampa bay buccaneers\n",
      "george w bush\n",
      "jp morgan chase\n",
      "lula da silva\n",
      "weapons of mass destruction\n",
      "tip of the iceberg\n",
      "german chancellor gerhard schroeder\n",
      "told the bbc news website\n",
      "asian tsunami disaster\n",
      "da vinci code\n",
      "prisoner of azkaban\n",
      "tells the story\n",
      "million dollar baby\n",
      "dead mans shoes\n",
      "shaun of the dead\n",
      "londons leicester square\n",
      "pirates of the caribbean\n",
      "eternal sunshine of the spotless mind\n",
      "passion of the christ\n",
      "howls moving castle\n",
      "robert de niro\n",
      "meet the fockers\n",
      "screen actors guild\n",
      "victims of the asian tsunami\n",
      "dismantle an atomic bomb\n",
      "manic street preachers\n",
      "rock n roll\n",
      "black eyed peas\n",
      "hall of fame\n",
      "rapper 50 cent\n",
      "t in the park\n",
      "phantom of the opera\n",
      "hepburn in the aviator\n",
      "bbc news website\n",
      "co-chairman liam fox\n",
      "partys spring conference\n",
      "leader charles kennedy\n",
      "foreign secretary jack straw\n",
      "miscarriages of justice\n",
      "affairs spokesman mark oaten\n",
      "secretary michael ancram\n",
      "secretary charles clarke\n",
      "spokesman mark oaten\n",
      "convention on human rights\n",
      "sir alan budd\n",
      "guy mansfield qc\n",
      "hands of extremists\n",
      "stamp duty threshold\n",
      "code of conduct\n",
      "archbishop of canterbury\n",
      "spring conference in gateshead\n",
      "prevention of terrorism\n",
      "political editor andrew marr\n",
      "concentration camp guard\n",
      "chairman matthew taylor\n",
      "mrs parker bowles\n",
      "leader roger knapman\n",
      "coordinator alan milburn\n",
      "iraqs weapons of mass destruction\n",
      "cabinet allies intervened\n",
      "telegraph journalist robert peston\n",
      "radio 4s today\n",
      "birmingham grand prix\n",
      "grand prix in birmingham\n",
      "association of athletics federations\n",
      "tests in tel aviv chicago\n",
      "faking a motorcycle accident\n",
      "tests in tel aviv\n",
      "coach christos tzekos\n",
      "lawyer gregory ioannidis\n",
      "ruud van nistelrooy\n",
      "boss jose mourinho\n",
      "jose antonio reyes\n",
      "white hart lane\n",
      "icing on the cake\n",
      "rbs six nations\n",
      "coach andy robinson\n",
      "coach bernard laporte\n",
      "referee jonathan kaplan\n",
      "sir clive woodward\n",
      "coach eddie osullivan\n",
      "coach mike ruddock\n",
      "roland de marigny\n",
      "r de marigny\n",
      "capt a persico\n",
      "ca del fava\n",
      "d dal maso\n",
      "juan carlos ferrero\n",
      "juan ignacio chela\n",
      "swiss army knife\n",
      "fibre optic cables\n",
      "pc pro magazine\n",
      "sonys playstation portable\n",
      "amazon.com holds a patent\n",
      "speed downlink packet\n",
      "downlink packet access\n",
      "electronic frontier foundation\n",
      "grand theft auto\n",
      "graham cluley senior\n",
      "los caballeros de plan\n",
      "blogs in existence\n",
      "gta san andreas\n",
      "hitting the shelves\n",
      "blue gene/l machine\n",
      "gameboyzz orchestra project\n"
     ]
    }
   ],
   "source": [
    "# Trigrams and above in the corpus (with `common_terms` set properly)\n",
    "seen_tokens = set()\n",
    "for document in phrased_docs:\n",
    "    for token in document.tokens:\n",
    "        if token.count(\" \") >= 2:\n",
    "            if token not in seen_tokens:\n",
    "                print(token)\n",
    "                seen_tokens.add(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-phrasing cleaning\n",
    "\n",
    "- Remove stop words (optional)\n",
    "- Remove purely numeric/non-alphabetic/single-character tokens\n",
    "  - Under the assumption that significant tokens, like the \"19\" in \"Covid 19\" or the \"11\" in \"Chapter 11 (bankruptcy)\" would have been picked up by the phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T02:40:40.488419Z",
     "start_time": "2020-09-10T02:40:40.484412Z"
    }
   },
   "outputs": [],
   "source": [
    "stopset = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T02:40:40.676003Z",
     "start_time": "2020-09-10T02:40:40.490990Z"
    }
   },
   "outputs": [],
   "source": [
    "# Stoplist based on TF-IDF\n",
    "# Not theoretically needed if using term weighting during the training process, but stopword\n",
    "# removal could still help with runtimes and interpretability\n",
    "\n",
    "\n",
    "import collections\n",
    "import math\n",
    "\n",
    "# Calculate IDF (For calculations of this size, the overhead of Pandas/Numpy is probably not worth it)\n",
    "df_dict = {}\n",
    "for doc in phrased_docs:\n",
    "    for token in set(doc.tokens):\n",
    "        if token in df_dict:\n",
    "            df_dict[token] += 1\n",
    "        else:\n",
    "            df_dict[token] = 1\n",
    "\n",
    "# While we're at it, we could remove document-level hapax legomena\n",
    "hapax = [token for token, count in df_dict.items() if count == 1]\n",
    "\n",
    "token_idf = {}\n",
    "for token in df_dict:\n",
    "    token_idf[token] = math.log(len(phrased_docs) / df_dict[token])\n",
    "\n",
    "\n",
    "def tf_idf(tokens, token_idf):\n",
    "    \"\"\"\n",
    "    Calculates the TF-IDF for each unique term in the given list of document tokens\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tokens: iterable of str\n",
    "        Tokens that make up a single document\n",
    "    token_idf: dict\n",
    "        Mapping of terms to their global IDF values\n",
    "    \"\"\"\n",
    "    token_tf_idf = {}\n",
    "    counts = collections.Counter(tokens)\n",
    "    for token in set(tokens):\n",
    "        token_tf = counts[token] / len(tokens)\n",
    "        # In particular, accessing a Pandas Series by string index is much slower than accessing a Dictionary by key\n",
    "        token_tf_idf[token] = token_tf * token_idf[token]\n",
    "    return token_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T02:40:41.081992Z",
     "start_time": "2020-09-10T02:40:40.677974Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords, ordered by TF-IDF:\n",
      "the, to, a, of, and, in, for, on, said, is, has, with, it, at, that, by, was, be, have, as, but, from, an, which, will, been, this, also, are, its, not, up, had, were, who, he, their, would, out, one, after, they, about, last, over, -\n",
      "(46)\n"
     ]
    }
   ],
   "source": [
    "# Thresholding\n",
    "\n",
    "# Number of terms with lowest TF-IDF scores to consider per document\n",
    "n_lowest = 50\n",
    "# The proportion of documents each of these terms must appear in to be considered a stopword\n",
    "stopword_proportion = 0.3\n",
    "\n",
    "per_doc_stopwords = []\n",
    "for doc in phrased_docs:\n",
    "    token_tf_idf = tf_idf(doc.tokens, token_idf)\n",
    "    # token_tf_idf is a dict of token -> score\n",
    "    scores = sorted(list(token_tf_idf.items()), key=lambda x: x[1])\n",
    "    lowest_n = [score[0] for score in scores[:n_lowest]]\n",
    "    per_doc_stopwords.append(set(lowest_n))\n",
    "\n",
    "# Check how many *stopword lists* each token appears in; this is more discriminative than\n",
    "# checking how many actual *documents* each token appears in instead\n",
    "stopword_df = {}\n",
    "for per_doc in per_doc_stopwords:\n",
    "    for stopword in per_doc:\n",
    "        if stopword in stopword_df:\n",
    "            stopword_df[stopword] += 1\n",
    "        else:\n",
    "            stopword_df[stopword] = 1\n",
    "\n",
    "total_docs = len(phrased_docs)\n",
    "final_stopwords = []\n",
    "for stopword, count in stopword_df.items():\n",
    "    if count / total_docs > stopword_proportion:\n",
    "        final_stopwords.append((stopword, count))\n",
    "final_stopwords.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# List of removed words sorted by TF-IDF (so in roughly decreasing order of commonness)\n",
    "removed_words = [stopword for stopword, count in final_stopwords]\n",
    "stopset = stopset.union(removed_words)\n",
    "\n",
    "print(\"Stopwords, ordered by TF-IDF:\")\n",
    "print(\", \".join(removed_words))\n",
    "print(f\"({len(removed_words)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T02:40:41.089975Z",
     "start_time": "2020-09-10T02:40:41.082998Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Also removing document-level hapaxes.\n"
     ]
    }
   ],
   "source": [
    "stopset = stopset.union(set(hapax))\n",
    "print(\"Also removing document-level hapaxes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T02:40:41.503586Z",
     "start_time": "2020-09-10T02:40:41.091979Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Also removing NLTK English stopwords.\n"
     ]
    }
   ],
   "source": [
    "import nltk.corpus\n",
    "\n",
    "nltk_stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "stopset = stopset.union(nltk_stopwords)\n",
    "print(\"Also removing NLTK English stopwords.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T02:40:41.516560Z",
     "start_time": "2020-09-10T02:40:41.504556Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain stopset:\n",
      "mr\n"
     ]
    }
   ],
   "source": [
    "domain_stopset = []\n",
    "\n",
    "# Corpus-specific high-frequency words\n",
    "domain_stopset += [\"mr\"]\n",
    "stopset = stopset.union(domain_stopset)\n",
    "print(\"Domain stopset:\")\n",
    "print(\", \".join(domain_stopset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T02:40:41.526558Z",
     "start_time": "2020-09-10T02:40:41.521563Z"
    }
   },
   "outputs": [],
   "source": [
    "# Whitelisting\n",
    "whitelist = []\n",
    "\n",
    "stopset -= set(whitelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T02:40:41.534590Z",
     "start_time": "2020-09-10T02:40:41.529556Z"
    }
   },
   "outputs": [],
   "source": [
    "def second_tokenise(tokens, stopset):\n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        if token in stopset or re.match(\"^[^a-z]+$\", token) or len(token) <= 1:\n",
    "            continue\n",
    "        new_tokens.append(token)\n",
    "\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T02:40:42.002617Z",
     "start_time": "2020-09-10T02:40:41.535557Z"
    }
   },
   "outputs": [],
   "source": [
    "final_docs = []\n",
    "for phrased_doc in phrased_docs:\n",
    "    final_docs.append(\n",
    "        phrased_doc._replace(tokens=second_tokenise(phrased_doc.tokens, stopset))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T02:40:42.240661Z",
     "start_time": "2020-09-10T02:40:42.003558Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('people', 1969),\n",
       " ('us', 1908),\n",
       " ('new', 1816),\n",
       " ('year', 1607),\n",
       " ('could', 1510),\n",
       " ('first', 1282),\n",
       " ('years', 1223),\n",
       " ('two', 1175),\n",
       " ('time', 1147),\n",
       " ('government', 1023),\n",
       " ('world', 1003),\n",
       " ('uk', 956),\n",
       " ('make', 927),\n",
       " ('best', 925),\n",
       " ('get', 890),\n",
       " ('made', 856),\n",
       " ('like', 838),\n",
       " ('film', 834),\n",
       " ('game', 831),\n",
       " ('many', 829)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the top remaining high-frequency words (for further cleaning if necessary)\n",
    "corpus_tf = {}\n",
    "for doc in final_docs:\n",
    "    doc_counts = collections.Counter(doc.tokens)\n",
    "\n",
    "    for token in set(doc.tokens):\n",
    "        if token in corpus_tf:\n",
    "            corpus_tf[token] += doc_counts[token]\n",
    "        else:\n",
    "            corpus_tf[token] = doc_counts[token]\n",
    "\n",
    "sorted(corpus_tf.items(), key=lambda x: x[1], reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T02:40:42.282572Z",
     "start_time": "2020-09-10T02:40:42.242635Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b7718ebae964a76b2704e9e8b5a7f50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='doc_id', max=2224), Output()), _dom_classes=('widget-int…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.show_final_doc(doc_id=0)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def show_final_doc(doc_id=0):\n",
    "    print(final_docs[doc_id].metadata)\n",
    "    print()\n",
    "    print(\" \".join(final_docs[doc_id].tokens))\n",
    "\n",
    "\n",
    "widgets.interact(show_final_doc, doc_id=(0, len(final_docs) - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T02:40:42.325572Z",
     "start_time": "2020-09-10T02:40:42.284572Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107 dupes.\n"
     ]
    }
   ],
   "source": [
    "# Simple deduplication\n",
    "seen_docs = set()\n",
    "dupe_count = 0\n",
    "deduped_docs = []\n",
    "for doc in final_docs:\n",
    "    if len(doc.tokens) == 0:\n",
    "        continue\n",
    "\n",
    "    # Cast the document tokens as a tuple so that we can use it as a deduplicating hash\n",
    "    doc_hash = tuple(doc.tokens)\n",
    "    if doc_hash in seen_docs:\n",
    "        # print(f\"Duplicate document: {doc[0]['filename']}\")\n",
    "        dupe_count += 1\n",
    "    else:\n",
    "        seen_docs.add(doc_hash)\n",
    "        deduped_docs.append(doc)\n",
    "\n",
    "print(f\"{dupe_count} dupes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to Ignis Corpus\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T02:40:43.506283Z",
     "start_time": "2020-09-10T02:40:42.328572Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = ignis.Corpus()\n",
    "\n",
    "for doc in deduped_docs:\n",
    "    if len(doc.tokens) < 5:\n",
    "        continue\n",
    "    corpus.add_doc(**doc._asdict())\n",
    "corpus.save(\"bbc.corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T02:40:44.133282Z",
     "start_time": "2020-09-10T02:40:43.508157Z"
    }
   },
   "outputs": [],
   "source": [
    "# And make sure it loads without errors as well.\n",
    "corpus = ignis.load_corpus(\"bbc.corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T02:40:44.166149Z",
     "start_time": "2020-09-10T02:40:44.135181Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2802e1a0b5044aa79f0188c348248524",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='index', max=2117), Output()), _dom_classes=('widget-inte…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.show_corpus_doc(index=0)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_doc_ids = list(corpus.documents.keys())\n",
    "\n",
    "\n",
    "def show_corpus_doc(index=0):\n",
    "    doc = corpus.documents[corpus_doc_ids[index]]\n",
    "    print(doc.metadata)\n",
    "    print()\n",
    "    print(\"-\" * 10)\n",
    "    print()\n",
    "    print(\" | \".join(doc.tokens))\n",
    "    print()\n",
    "    print(\"-\" * 10)\n",
    "    print()\n",
    "\n",
    "    # Jupyter notebooks will interpret anything between $ signs as LaTeX formulae when rendering HTML output,\n",
    "    # so we need to replace them with escaped $ signs (only in Jupyter environments)\n",
    "    display_str = doc.display_str.replace(\"$\", r\"\\$\")\n",
    "    display(HTML(display_str))\n",
    "\n",
    "\n",
    "widgets.interact(show_corpus_doc, index=(0, len(corpus_doc_ids) - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0176bce07868479ba1260626d7e96ae2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "SliderStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "0c6041bc44c44b2793dab67ad5b0794b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "0f6858aff9a6418893b5db2b75af667d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "13eb7efb1d294d8390b86540aaaeeb07": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1f898984afc444e49b13dc17023418de": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "23c2e3055cdd4b0fb5ade54f47417ae4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "VBoxModel",
      "state": {
       "_dom_classes": [
        "widget-interact"
       ],
       "children": [
        "IPY_MODEL_9636b5d65c6c45df80d319d5ae4775a8",
        "IPY_MODEL_963c8fb4d7e24e14b38a4ef5f44001e5"
       ],
       "layout": "IPY_MODEL_b5cc154c0da84a9b95e463fe55e0d966"
      }
     },
     "2e381aa08f0f498f97bdc2a2aae009da": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "VBoxModel",
      "state": {
       "_dom_classes": [
        "widget-interact"
       ],
       "children": [
        "IPY_MODEL_3a292d4848e340bbb03efcb44b4c5546",
        "IPY_MODEL_5259f03c2a4f4730902ea770b0867c84"
       ],
       "layout": "IPY_MODEL_0c6041bc44c44b2793dab67ad5b0794b"
      }
     },
     "300ddb0791ce4396893229818dff50e8": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_510ec16b4b0c460395beef57d1b19385",
       "outputs": [
        {
         "name": "stdout",
         "output_type": "stream",
         "text": "{'filename': 'data/bbc/business/001.txt'}\n\nAd sales boost Time Warner profit\n\nQuarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (Â£600m) for the three months to December, from $639m year-earlier.\n\nThe firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales. TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn. Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL.\n\nTime Warner said on Friday that it now owns 8% of search-engine Google. But its own internet business, AOL, had has mixed fortunes. It lost 464,000 subscribers in the fourth quarter profits were lower than in the preceding three quarters. However, the company said AOL's underlying profit before exceptional items rose 8% on the back of stronger internet advertising revenues. It hopes to increase subscribers by offering the online service free to TimeWarner internet customers and will try to sign up AOL's existing customers for high-speed broadband. TimeWarner also has to restate 2000 and 2003 results following a probe by the US Securities Exchange Commission (SEC), which is close to concluding.\n\nTime Warner's fourth quarter profits were slightly better than analysts' expectations. But its film division saw profits slump 27% to $284m, helped by box-office flops Alexander and Catwoman, a sharp contrast to year-earlier, when the third and final film in the Lord of the Rings trilogy boosted results. For the full-year, TimeWarner posted a profit of $3.36bn, up 27% from its 2003 performance, while revenues grew 6.4% to $42.09bn. \"Our financial performance was strong, meeting or exceeding all of our full-year objectives and greatly enhancing our flexibility,\" chairman and chief executive Richard Parsons said. For 2005, TimeWarner is projecting operating earnings growth of around 5%, and also expects higher revenue and wider profit margins.\n\nTimeWarner is to restate its accounts as part of efforts to resolve an inquiry into AOL by US market regulators. It has already offered to pay $300m to settle charges, in a deal that is under review by the SEC. The company said it was unable to estimate the amount it needed to set aside for legal reserves, which it previously set at $500m. It intends to adjust the way it accounts for a deal with German music publisher Bertelsmann's purchase of a stake in AOL Europe, which it had reported as advertising revenue. It will now book the sale of its stake in AOL Europe as a loss on the value of that stake.\n\n"
        }
       ]
      }
     },
     "363d527b513e4ffab5f46f0a9cc2737b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "3a292d4848e340bbb03efcb44b4c5546": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntSliderModel",
      "state": {
       "description": "doc_id",
       "layout": "IPY_MODEL_9ed5f929f05949ca81047f3a84ae1e84",
       "max": 2224,
       "style": "IPY_MODEL_5c718fb3a6c54077b79a5a284570c0e0"
      }
     },
     "4a3ed32352254eeebe3368a117a9a382": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "SliderStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "4df391293247441dba818e14d38af8ea": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "510ec16b4b0c460395beef57d1b19385": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5259f03c2a4f4730902ea770b0867c84": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_1f898984afc444e49b13dc17023418de",
       "outputs": [
        {
         "name": "stdout",
         "output_type": "stream",
         "text": "{'filename': 'data/bbc/business/001.txt'}\n\nad sales boost time warner profit quarterly profits at us media giant timewarner jumped to $1.13bn â£600m for the three months to december from $639m year-earlier the firm which is now one of the biggest investors in google benefited from sales of high-speed internet connections and higher advert sales timewarner said fourth quarter sales rose to $11.1bn from $10.9bn its profits were buoyed by one-off gains which offset profit dip at warner bros and less users for aol time warner said on friday that it now owns of search-engine google but its own internet business aol had has mixed fortunes it lost subscribers in the fourth quarter profits were lower than in the preceding three quarters however the company said aols underlying profit before exceptional items rose on the back of stronger internet advertising revenues it hopes to increase subscribers by offering the online service free to timewarner internet customers and will try to sign up aols existing customers for high-speed broadband timewarner also has to restate and results following probe by the us securities exchange commission sec which is close to concluding time warners fourth quarter profits were slightly better than analysts expectations but its film division saw profits slump to $284m helped by box-office flops alexander and catwoman sharp contrast to year-earlier when the third and final film in the lord_of_the_rings trilogy boosted results for the full-year timewarner posted profit of $3.36bn up from its performance while revenues grew to $42.09bn our financial performance was strong meeting or exceeding all of our full-year objectives and greatly enhancing our flexibility chairman and chief_executive richard parsons said for timewarner is projecting operating earnings growth of around and also expects higher revenue and wider profit margins timewarner is to restate its accounts as part of efforts to resolve an inquiry into aol by us market regulators it has already offered to pay $300m to settle charges in deal that is under review by the sec the company said it was unable to estimate the amount it needed to set aside for legal reserves which it previously set at $500m it intends to adjust the way it accounts for deal with german music publisher bertelsmanns purchase of stake in aol europe which it had reported as advertising revenue it will now book the sale of its stake in aol europe as loss on the value of that stake\n"
        }
       ]
      }
     },
     "596c9f0a5ee54976b5c305733046eb20": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "VBoxModel",
      "state": {
       "_dom_classes": [
        "widget-interact"
       ],
       "children": [
        "IPY_MODEL_72c64a752960491d904da0ddbdf4798b",
        "IPY_MODEL_972d09b6a6024423a6de54b5f13114a5"
       ],
       "layout": "IPY_MODEL_c8473dedaa18462dbd9ed2df63cffc33"
      }
     },
     "5c718fb3a6c54077b79a5a284570c0e0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "SliderStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "6e2d7a2b5ae14a82bc6493af5230b988": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "72c64a752960491d904da0ddbdf4798b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntSliderModel",
      "state": {
       "description": "doc_id",
       "layout": "IPY_MODEL_4df391293247441dba818e14d38af8ea",
       "max": 2224,
       "style": "IPY_MODEL_4a3ed32352254eeebe3368a117a9a382"
      }
     },
     "9636b5d65c6c45df80d319d5ae4775a8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntSliderModel",
      "state": {
       "description": "doc_id",
       "layout": "IPY_MODEL_13eb7efb1d294d8390b86540aaaeeb07",
       "max": 2224,
       "style": "IPY_MODEL_0176bce07868479ba1260626d7e96ae2"
      }
     },
     "963c8fb4d7e24e14b38a4ef5f44001e5": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_c7609d692cbd4e8e967cb8cbd2357b69",
       "outputs": [
        {
         "name": "stdout",
         "output_type": "stream",
         "text": "{'filename': 'data/bbc/business/001.txt'}\n\nad sales boost time warner profit quarterly profits at us media giant timewarner jumped 76% to $1.13bn â£600m for the three months to december from $639m year-earlier the firm which is now one of the biggest investors in google benefited from sales of high-speed internet connections and higher advert sales timewarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn its profits were buoyed by one-off gains which offset a profit dip at warner bros and less users for aol time warner said on friday that it now owns 8% of search-engine google but its own internet business aol had has mixed fortunes it lost 464,000 subscribers in the fourth quarter profits were lower than in the preceding three quarters however the company said aols underlying profit before exceptional items rose 8% on the back of stronger internet advertising revenues it hopes to increase subscribers by offering the online service free to timewarner internet customers and will try to sign up aols existing customers for high-speed broadband timewarner also has to restate 2000 and 2003 results following a probe by the us securities exchange commission sec which is close to concluding time warners fourth quarter profits were slightly better than analysts expectations but its film division saw profits slump 27% to $284m helped by box-office flops alexander and catwoman a sharp contrast to year-earlier when the third and final film in the lord_of_the_rings trilogy boosted results for the full-year timewarner posted a profit of $3.36bn up 27% from its 2003 performance while revenues grew 6.4% to $42.09bn our financial performance was strong meeting or exceeding all of our full-year objectives and greatly enhancing our flexibility chairman and chief_executive richard parsons said for 2005 timewarner is projecting operating earnings growth of around 5% and also expects higher revenue and wider profit margins timewarner is to restate its accounts as part of efforts to resolve an inquiry into aol by us market regulators it has already offered to pay $300m to settle charges in a deal that is under review by the sec the company said it was unable to estimate the amount it needed to set aside for legal reserves which it previously set at $500m it intends to adjust the way it accounts for a deal with german music publisher bertelsmanns purchase of a stake in aol europe which it had reported as advertising revenue it will now book the sale of its stake in aol europe as a loss on the value of that stake\n"
        }
       ]
      }
     },
     "972d09b6a6024423a6de54b5f13114a5": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_6e2d7a2b5ae14a82bc6493af5230b988",
       "outputs": [
        {
         "name": "stdout",
         "output_type": "stream",
         "text": "{'filename': 'data/bbc/business/001.txt'}\n\nad sales boost time warner profit quarterly profits at us media giant timewarner jumped 76% to $1.13bn â£600m for the three months to december from $639m year-earlier the firm which is now one of the biggest investors in google benefited from sales of high-speed internet connections and higher advert sales timewarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn its profits were buoyed by one-off gains which offset a profit dip at warner bros and less users for aol time warner said on friday that it now owns 8% of search-engine google but its own internet business aol had has mixed fortunes it lost 464,000 subscribers in the fourth quarter profits were lower than in the preceding three quarters however the company said aols underlying profit before exceptional items rose 8% on the back of stronger internet advertising revenues it hopes to increase subscribers by offering the online service free to timewarner internet customers and will try to sign up aols existing customers for high-speed broadband timewarner also has to restate 2000 and 2003 results following a probe by the us securities exchange commission sec which is close to concluding time warners fourth quarter profits were slightly better than analysts expectations but its film division saw profits slump 27% to $284m helped by box-office flops alexander and catwoman a sharp contrast to year-earlier when the third and final film in the lord of the rings trilogy boosted results for the full-year timewarner posted a profit of $3.36bn up 27% from its 2003 performance while revenues grew 6.4% to $42.09bn our financial performance was strong meeting or exceeding all of our full-year objectives and greatly enhancing our flexibility chairman and chief executive richard parsons said for 2005 timewarner is projecting operating earnings growth of around 5% and also expects higher revenue and wider profit margins timewarner is to restate its accounts as part of efforts to resolve an inquiry into aol by us market regulators it has already offered to pay $300m to settle charges in a deal that is under review by the sec the company said it was unable to estimate the amount it needed to set aside for legal reserves which it previously set at $500m it intends to adjust the way it accounts for a deal with german music publisher bertelsmanns purchase of a stake in aol europe which it had reported as advertising revenue it will now book the sale of its stake in aol europe as a loss on the value of that stake\n"
        }
       ]
      }
     },
     "9ed5f929f05949ca81047f3a84ae1e84": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b5cc154c0da84a9b95e463fe55e0d966": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "bc3c0ab7399b4b16b5d34f7b6307998b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "VBoxModel",
      "state": {
       "_dom_classes": [
        "widget-interact"
       ],
       "children": [
        "IPY_MODEL_e4e663e13b1c4bb4a38edf7e816b4937",
        "IPY_MODEL_300ddb0791ce4396893229818dff50e8"
       ],
       "layout": "IPY_MODEL_363d527b513e4ffab5f46f0a9cc2737b"
      }
     },
     "c5cdae633e8d419f8e138445d94e8bbc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "SliderStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "c7609d692cbd4e8e967cb8cbd2357b69": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c8473dedaa18462dbd9ed2df63cffc33": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e4e663e13b1c4bb4a38edf7e816b4937": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntSliderModel",
      "state": {
       "description": "doc_id",
       "layout": "IPY_MODEL_0f6858aff9a6418893b5db2b75af667d",
       "max": 2224,
       "style": "IPY_MODEL_c5cdae633e8d419f8e138445d94e8bbc"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}