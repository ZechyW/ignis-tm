{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Text Pre-processing and Topic Modelling\n",
    "======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T07:52:12.872170Z",
     "start_time": "2020-06-05T07:52:10.084020Z"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import math\n",
    "import re\n",
    "import threading\n",
    "import time\n",
    "\n",
    "import gensim\n",
    "import nltk\n",
    "import pyLDAvis\n",
    "import tomotopy as tp\n",
    "import tqdm\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T07:52:12.924136Z",
     "start_time": "2020-06-05T07:52:12.874121Z"
    }
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data ingestion\n",
    "--------------------\n",
    "\n",
    "We will track the contents and filename of each document, then tokenise them all and feed them into an `ignis.Corpus`.\n",
    "\n",
    "We should, by all accounts, actually be preparing a separate text cleaning function and running the raw text through it immediately, but this way we can see the effects of each step of the data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T07:52:12.941114Z",
     "start_time": "2020-06-05T07:52:12.925132Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_files = glob.glob(\"./data/bbc/*/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T07:52:13.192033Z",
     "start_time": "2020-06-05T07:52:12.942121Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2225/2225 [00:00<00:00, 9233.96it/s]\n"
     ]
    }
   ],
   "source": [
    "raw_docs = []\n",
    "for file in tqdm.tqdm(raw_files):\n",
    "    filename = pathlib.Path(file).as_posix()\n",
    "    metadata = {\n",
    "        \"filename\": filename\n",
    "    }\n",
    "    \n",
    "    with open(file) as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    raw_docs.append([metadata, text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T07:52:13.223029Z",
     "start_time": "2020-06-05T07:52:13.193077Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f374f892daf48be94b31bbfd5601747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='doc_id', max=2224), Output()), _dom_classes=('widget-int…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.show_raw_doc(doc_id=0)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def show_raw_doc(doc_id=0):\n",
    "    print(raw_docs[doc_id][0])\n",
    "    print()\n",
    "    print(raw_docs[doc_id][1])\n",
    "\n",
    "widgets.interact(show_raw_doc, doc_id=(0, len(raw_docs) - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text pre-processing and tokenisation\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive tokenisation (by whitespace)\n",
    "- Case folding\n",
    "- Strip leading/trailing non-informative punctuation from tokens\n",
    "- Remove single apostrophes\n",
    "- Remove single brackets within words\n",
    "  - For dealing with cases like \"the recipient(s)\" -- Which will get tokenised to \"the recipient(s\" otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T07:52:13.236061Z",
     "start_time": "2020-06-05T07:52:13.226041Z"
    }
   },
   "outputs": [],
   "source": [
    "strip_punctuation = \"'\\\"()[]<>?!,.:;/|_\"\n",
    "bracket_pairs = [\n",
    "    [\"(\", \")\"],\n",
    "    [\"[\", \"]\"],\n",
    "]\n",
    "\n",
    "\n",
    "def naive_tokenise(doc):\n",
    "    \"\"\"\n",
    "    Naively tokenises a document.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The document as a string of space-separated tokens\n",
    "    \"\"\"\n",
    "    new_tokens = []\n",
    "    \n",
    "    tokens = doc.split()\n",
    "    for token in tokens:\n",
    "        token = token.casefold()\n",
    "        token = token.strip(strip_punctuation)\n",
    "        token = token.replace(\"'\", \"\")\n",
    "        \n",
    "        for bracket_pair in bracket_pairs:\n",
    "            if bracket_pair[0] in token and bracket_pair[1] not in token:\n",
    "                token = token.replace(bracket_pair[0], \"\")\n",
    "            if bracket_pair[1] in token and bracket_pair[0] not in token:\n",
    "                token = token.replace(bracket_pair[1], \"\")\n",
    "        \n",
    "        if token != \"\":\n",
    "            new_tokens.append(token)\n",
    "            \n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T07:52:13.246024Z",
     "start_time": "2020-06-05T07:52:13.239024Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'a',\n",
       " 't(e)st',\n",
       " 'of',\n",
       " 'the',\n",
       " 'systems',\n",
       " 'tokenisation',\n",
       " 'operations']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_tokenise('This is a t(e)st of the system\\'s \"tokenisation\" operation(s).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T07:52:13.817176Z",
     "start_time": "2020-06-05T07:52:13.248022Z"
    }
   },
   "outputs": [],
   "source": [
    "naive_docs = []\n",
    "for raw_doc in raw_docs:\n",
    "    naive_docs.append([raw_doc[0], naive_tokenise(raw_doc[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T07:52:13.847831Z",
     "start_time": "2020-06-05T07:52:13.818170Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9edf1d753c684252a3814c22ded58ccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='doc_id', max=2224), Output()), _dom_classes=('widget-int…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.show_naive_doc(doc_id=0)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def show_naive_doc(doc_id=0):\n",
    "    print(naive_docs[doc_id][0])\n",
    "    print()\n",
    "    print(\" \".join(naive_docs[doc_id][1]))\n",
    "\n",
    "widgets.interact(show_naive_doc, doc_id=(0, len(naive_docs) - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automated n-gram detection\n",
    "\n",
    "Chunk into significant bigrams/trigrams based on collocation frequency\n",
    "\n",
    "(N.B.: Gensim implies that the input to the Phraser should be a list of single sentences, but we will feed it a list of documents instead.)\n",
    "\n",
    "- Min count: How many documents the n-grams need to appear in\n",
    "\n",
    "- Scoring: \"default\" or \"npmi\"\n",
    "\n",
    "- Threshold: Intuitively, higher threshold means fewer phrases.\n",
    "  - With the default scorer, this is greater than or equal to 0; with the NPMI scorer, this is in the range -1 to 1.\n",
    "\n",
    "- Common terms: These terms will be ignored if they come between normal words.\n",
    "  - E.g., if `common_terms` includes the word \"of\", then when the phraser sees \"Wheel of Fortune\" it actually evaluates _\"Wheel Fortune\"_ as an n-gram, putting \"of\" back in only at the output level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T07:52:13.853830Z",
     "start_time": "2020-06-05T07:52:13.849808Z"
    }
   },
   "outputs": [],
   "source": [
    "min_count = 5\n",
    "scoring = \"npmi\"\n",
    "# We want a relatively high threshold so that we don't start littering spurious n-grams all over our corpus, diluting our results.\n",
    "# E.g., we want \"Lord_of_the_Rings\", but not \"slightly_better_than_analysts\"\n",
    "threshold = 0.7\n",
    "common_terms = [\"a\", \"an\", \"the\", \"of\", \"on\", \"in\", \"at\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This could take a while, so set up a threaded function with a basic progress indicator in the main thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T07:52:13.862802Z",
     "start_time": "2020-06-05T07:52:13.855803Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_trigrams(docs, results):\n",
    "    # Build, finalise, and apply the bigram model\n",
    "    bigram_model = gensim.models.Phrases(\n",
    "        docs,\n",
    "        min_count=min_count,\n",
    "        threshold=threshold,\n",
    "        scoring=scoring,\n",
    "        common_terms=common_terms,\n",
    "    )\n",
    "    bigram_model = gensim.models.phrases.Phraser(bigram_model)\n",
    "    \n",
    "    bigram_docs = bigram_model[docs]\n",
    "    \n",
    "    # Repeat to get trigrams\n",
    "    trigram_model = gensim.models.Phrases(\n",
    "        bigram_docs,\n",
    "        min_count=min_count,\n",
    "        threshold=threshold,\n",
    "        scoring=scoring,\n",
    "        common_terms=common_terms,\n",
    "    )\n",
    "    trigram_model = gensim.models.phrases.Phraser(trigram_model)\n",
    "    \n",
    "    trigram_docs = trigram_model[docs]\n",
    "    \n",
    "    results[0] = trigram_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T07:52:26.677290Z",
     "start_time": "2020-06-05T07:52:13.864803Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding trigrams . . . . . . . . . Done. (11.202s)\n"
     ]
    }
   ],
   "source": [
    "print(\"Finding trigrams\", flush=True, end=\"\")\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "# Just send the textual content through the Phraser, not the document metadata\n",
    "for_phrasing = [naive_doc[1] for naive_doc in naive_docs]\n",
    "\n",
    "# Will contain the documents after trigram processing\n",
    "results = [None]\n",
    "t = threading.Thread(target=find_trigrams, args=(for_phrasing, results))\n",
    "t.start()\n",
    "\n",
    "progress_countdown = 1.0\n",
    "\n",
    "while t.isAlive():\n",
    "    time.sleep(0.1)\n",
    "    progress_countdown -= 0.1\n",
    "    if progress_countdown <= 0:\n",
    "        print(\" .\", flush=True, end=\"\")\n",
    "        progress_countdown = 1\n",
    "\n",
    "elapsed = time.perf_counter() - start_time\n",
    "print(f\" Done. ({elapsed:.3f}s)\")\n",
    "\n",
    "after_phrasing = results[0]\n",
    "\n",
    "# Put metadata back in\n",
    "phrased_docs = []\n",
    "for index, tokens in enumerate(after_phrasing):\n",
    "    phrased_docs.append([naive_docs[index][0], tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T07:52:26.711236Z",
     "start_time": "2020-06-05T07:52:26.678249Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa0e819114484264b32c10a509938dfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='doc_id', max=2224), Output()), _dom_classes=('widget-int…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.show_phrased_doc(doc_id=0)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def show_phrased_doc(doc_id=0):\n",
    "    print(phrased_docs[doc_id][0])\n",
    "    print()\n",
    "    print(\" \".join(phrased_docs[doc_id][1]))\n",
    "\n",
    "widgets.interact(show_phrased_doc, doc_id=(0, len(phrased_docs) - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-phrasing cleaning\n",
    "\n",
    "- Remove stop words (optional)\n",
    "- Remove purely numeric/non-alphabetic/single-character tokens\n",
    "  - Under the assumption that significant tokens, like the \"19\" in \"Covid 19\", would have been picked up by the phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T07:52:26.720235Z",
     "start_time": "2020-06-05T07:52:26.713217Z"
    }
   },
   "outputs": [],
   "source": [
    "# stopset = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "# Not needed if using term weighting\n",
    "stopset = []\n",
    "\n",
    "\n",
    "def second_tokenise(tokens):\n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        if token in stopset or re.match(\"^[^a-z]+$\", token) or len(token) <= 1:\n",
    "            continue\n",
    "        new_tokens.append(token)\n",
    "\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T07:52:27.521115Z",
     "start_time": "2020-06-05T07:52:26.722223Z"
    }
   },
   "outputs": [],
   "source": [
    "final_docs = []\n",
    "for phrased_doc in phrased_docs:\n",
    "    final_docs.append([phrased_doc[0], second_tokenise(phrased_doc[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T07:52:27.553938Z",
     "start_time": "2020-06-05T07:52:27.522117Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "202db32db4a84d75924e7e6b07fbc65f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='doc_id', max=2224), Output()), _dom_classes=('widget-int…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.show_final_doc(doc_id=0)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def show_final_doc(doc_id=0):\n",
    "    print(final_docs[doc_id][0])\n",
    "    print()\n",
    "    print(\" \".join(final_docs[doc_id][1]))\n",
    "\n",
    "widgets.interact(show_final_doc, doc_id=(0, len(final_docs) - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training (LDA)\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add processed docs to the LDA model and train it.\n",
    "\n",
    "The random seed and parallelisation can both affect results, so setting the seed and number of workers is necessary for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T07:52:27.569965Z",
     "start_time": "2020-06-05T07:52:27.554971Z"
    }
   },
   "outputs": [],
   "source": [
    "import ignis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T07:52:27.601958Z",
     "start_time": "2020-06-05T07:52:27.572937Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = ignis.Corpus()\n",
    "\n",
    "for metadata, tokens in final_docs:\n",
    "    corpus.add_doc(metadata, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T07:52:44.532816Z",
     "start_time": "2020-06-05T07:52:27.603927Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LDA model:\n",
      "{'term_weighting': 'idf', 'k': 20, 'seed': 11399, 'workers': 8, 'parallel_scheme': 'default', 'iterations': 1000, 'update_every': 100, 'until_max_ll': False, 'max_extra_iterations': 5000, 'verbose': True, 'tw': <TermWeight.IDF: 1>, 'parallel': <ParallelScheme.DEFAULT: 0>}\n",
      "\n",
      "Iteration: 100\tLog-likelihood: -21.27849047035462\tTime: 2.284s\n",
      "Iteration: 200\tLog-likelihood: -20.933526954007277\tTime: 2.296s\n",
      "Iteration: 300\tLog-likelihood: -20.76408325795117\tTime: 2.135s\n",
      "Iteration: 400\tLog-likelihood: -20.681196461159733\tTime: 1.744s\n",
      "Iteration: 500\tLog-likelihood: -20.62150393437183\tTime: 1.386s\n",
      "Iteration: 600\tLog-likelihood: -20.5748602387642\tTime: 1.385s\n",
      "Iteration: 700\tLog-likelihood: -20.552054316530977\tTime: 1.381s\n",
      "Iteration: 800\tLog-likelihood: -20.51634455334242\tTime: 1.373s\n",
      "Iteration: 900\tLog-likelihood: -20.48474095778886\tTime: 1.380s\n",
      "Iteration: 1000\tLog-likelihood: -20.45425931609133\tTime: 1.369s\n",
      "Model training complete. (16.782s)\n"
     ]
    }
   ],
   "source": [
    "results = ignis.train_model(corpus, model_type=\"lda\", model_options={\"verbose\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T07:53:11.706934Z",
     "start_time": "2020-06-05T07:53:08.357912Z"
    }
   },
   "outputs": [],
   "source": [
    "results.save(\"test.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T07:52:44.537819Z",
     "start_time": "2020-06-05T07:52:44.533816Z"
    }
   },
   "outputs": [],
   "source": [
    "# Persistence\n",
    "model_seed = 11399\n",
    "num_workers = 12\n",
    "\n",
    "# Model options\n",
    "model_file = \"model.bin\"\n",
    "num_topics = 20\n",
    "\n",
    "# Training iterations\n",
    "load_saved_model = False\n",
    "train_batch = 100\n",
    "train_total = 1000\n",
    "\n",
    "# Extended training\n",
    "train_until_min_ll = True\n",
    "max_iterations = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T07:52:44.552895Z",
     "start_time": "2020-06-05T07:52:44.540361Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# if load_saved_model:\n",
    "#     model = tp.LDAModel.load(model_file)\n",
    "#     print(f\"Loaded from '{model_file}'.\")\n",
    "# else:\n",
    "#     model = tp.LDAModel(tw=tp.TermWeight.IDF, seed=model_seed, k=num_topics)\n",
    "\n",
    "#     for doc in tqdm.tqdm(docs):\n",
    "#         model.add_doc(doc)\n",
    "\n",
    "#     model.train(0, workers=num_workers, parallel=tp.ParallelScheme.DEFAULT)\n",
    "#     print(\n",
    "#         f\"Num docs: {len(model.docs)}, Vocab size: {model.num_vocabs}, \"\n",
    "#         f\"Num words: {model.num_words}\"\n",
    "#     )\n",
    "#     print(f\"Removed top words: {model.removed_top_words}\")\n",
    "\n",
    "#     print(\"Training model...\", flush=True)\n",
    "\n",
    "#     try:\n",
    "#         for i in range(0, train_total, train_batch):\n",
    "#             start_time = time.perf_counter()\n",
    "#             model.train(\n",
    "#                 train_batch, workers=num_workers, parallel=tp.ParallelScheme.DEFAULT\n",
    "#             )\n",
    "#             elapsed = time.perf_counter() - start_time\n",
    "#             print(\n",
    "#                 f\"Iteration: {i + train_batch}\\tLog-likelihood: {model.ll_per_word}\\t\"\n",
    "#                 f\"Time: {elapsed:.3f}s\",\n",
    "#                 flush=True,\n",
    "#             )\n",
    "#     except KeyboardInterrupt:\n",
    "#         print(\"Stopping train sequence.\")\n",
    "#     model.save(model_file)\n",
    "#     print(f\"Saved to '{model_file}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T07:52:44.560891Z",
     "start_time": "2020-06-05T07:52:44.554893Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# if train_until_min_ll:\n",
    "#     print(\"Continuing to train until minimum log-likelihood...\")\n",
    "#     print(\"(N.B.: This may not correlate with increased human interpretability)\")\n",
    "#     last_ll = model.ll_per_word\n",
    "#     i = 0\n",
    "#     consecutive_loss = 0\n",
    "\n",
    "#     while True:\n",
    "#         try:\n",
    "#             start_time = time.perf_counter()\n",
    "#             model.train(\n",
    "#                 train_batch, workers=num_workers, parallel=tp.ParallelScheme.DEFAULT\n",
    "#             )\n",
    "#             i += train_batch\n",
    "#             elapsed = time.perf_counter() - start_time\n",
    "#             print(\n",
    "#                 f\"Iteration: {i}\\tLog-likelihood: {model.ll_per_word}\\t\"\n",
    "#                 f\"Time: {elapsed:.3f}s\",\n",
    "#                 flush=True,\n",
    "#             )\n",
    "\n",
    "#             if model.ll_per_word < last_ll:\n",
    "#                 consecutive_loss += 1\n",
    "#             else:\n",
    "#                 consecutive_loss = 0\n",
    "#                 model.save(model_file)\n",
    "#             last_ll = model.ll_per_word\n",
    "\n",
    "#             if consecutive_loss == 2 or i >= max_iterations:\n",
    "#                 break\n",
    "\n",
    "#         except KeyboardInterrupt:\n",
    "#             print(\"Stopping extended train sequence.\")\n",
    "#             break\n",
    "\n",
    "#     model = tp.LDAModel.load(model_file)\n",
    "#     print(f\"Best recent model saved at '{model_file}' (LL: {model.ll_per_word}).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Topic labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T07:52:44.565894Z",
     "start_time": "2020-06-05T07:52:44.562893Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# print(\"Extracting suggested topic labels...\", flush=True)\n",
    "# # extractor = tp.label.PMIExtractor(min_cf=10, min_df=5, max_len=5, max_cand=10000)\n",
    "# extractor = tp.label.PMIExtractor(min_cf=5, min_df=3, max_len=5, max_cand=20000)\n",
    "# candidates = extractor.extract(model)\n",
    "# # labeler = tp.label.FoRelevance(model, candidates, min_df=5, smoothing=1e-2,\n",
    "# # mu=0.25)\n",
    "# labeler = tp.label.FoRelevance(\n",
    "#     model, candidates, min_df=3, smoothing=1e-2, mu=0.25, workers=num_workers\n",
    "# )\n",
    "# print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print results\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T07:52:44.572893Z",
     "start_time": "2020-06-05T07:52:44.567892Z"
    }
   },
   "outputs": [],
   "source": [
    "# def print_topic(topic_id):\n",
    "#     # Labels\n",
    "#     labels = \", \".join(\n",
    "#         label for label, score in labeler.get_topic_labels(topic_id, top_n=10)\n",
    "#     )\n",
    "#     print(f\"Suggested labels: {labels}\")\n",
    "\n",
    "#     # Print this topic\n",
    "#     words_probs = model.get_topic_words(topic_id, top_n=10)\n",
    "#     words = [x[0] for x in words_probs]\n",
    "\n",
    "#     words = \", \".join(words)\n",
    "#     print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T07:52:44.578891Z",
     "start_time": "2020-06-05T07:52:44.575896Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for k in range(model.k):\n",
    "#     print(f\"[Topic {k+1}]\")\n",
    "#     print_topic(k)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise\n",
    "--------\n",
    "- Present data in the format expected by pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T07:52:44.585890Z",
     "start_time": "2020-06-05T07:52:44.579889Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model_data = {\n",
    "#     \"topic_term_dists\": [model.get_topic_word_dist(k) for k in range(model.k)],\n",
    "#     \"doc_topic_dists\": [model.docs[n].get_topic_dist() for n in range(len(model.docs))],\n",
    "#     \"doc_lengths\": [len(model.docs[n].words) for n in range(len(model.docs))],\n",
    "#     \"vocab\": model.vocabs,\n",
    "#     \"term_frequency\": model.vocab_freq,\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this could take a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T07:52:44.592886Z",
     "start_time": "2020-06-05T07:52:44.587892Z"
    }
   },
   "outputs": [],
   "source": [
    "# def prepare_vis(model_data, results):\n",
    "#     vis_data = pyLDAvis.prepare(**model_data)\n",
    "#     results[0] = vis_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T07:52:44.599885Z",
     "start_time": "2020-06-05T07:52:44.594888Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(\"Preparing LDA visualisation\", flush=True, end=\"\")\n",
    "\n",
    "# results = [None]\n",
    "# t = threading.Thread(target=prepare_vis, args=(model_data, results))\n",
    "# t.start()\n",
    "\n",
    "# progress_countdown = 1.0\n",
    "\n",
    "# while t.isAlive():\n",
    "#     time.sleep(0.1)\n",
    "#     progress_countdown -= 0.1\n",
    "#     if progress_countdown <= 0:\n",
    "#         print(\" .\", flush=True, end=\"\")\n",
    "#         progress_countdown = 1\n",
    "\n",
    "# print(\" Done.\")\n",
    "\n",
    "# vis_data = results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T07:52:44.605886Z",
     "start_time": "2020-06-05T07:52:44.600887Z"
    }
   },
   "outputs": [],
   "source": [
    "# pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate\n",
    "--------\n",
    "- See what the main topics might be, slice initial corpus and re-run LDA to get sub-topics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0176bce07868479ba1260626d7e96ae2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "SliderStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "0c6041bc44c44b2793dab67ad5b0794b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "0f6858aff9a6418893b5db2b75af667d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "13eb7efb1d294d8390b86540aaaeeb07": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1f898984afc444e49b13dc17023418de": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "23c2e3055cdd4b0fb5ade54f47417ae4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "VBoxModel",
      "state": {
       "_dom_classes": [
        "widget-interact"
       ],
       "children": [
        "IPY_MODEL_9636b5d65c6c45df80d319d5ae4775a8",
        "IPY_MODEL_963c8fb4d7e24e14b38a4ef5f44001e5"
       ],
       "layout": "IPY_MODEL_b5cc154c0da84a9b95e463fe55e0d966"
      }
     },
     "2e381aa08f0f498f97bdc2a2aae009da": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "VBoxModel",
      "state": {
       "_dom_classes": [
        "widget-interact"
       ],
       "children": [
        "IPY_MODEL_3a292d4848e340bbb03efcb44b4c5546",
        "IPY_MODEL_5259f03c2a4f4730902ea770b0867c84"
       ],
       "layout": "IPY_MODEL_0c6041bc44c44b2793dab67ad5b0794b"
      }
     },
     "300ddb0791ce4396893229818dff50e8": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_510ec16b4b0c460395beef57d1b19385",
       "outputs": [
        {
         "name": "stdout",
         "output_type": "stream",
         "text": "{'filename': 'data/bbc/business/001.txt'}\n\nAd sales boost Time Warner profit\n\nQuarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (Â£600m) for the three months to December, from $639m year-earlier.\n\nThe firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales. TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn. Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL.\n\nTime Warner said on Friday that it now owns 8% of search-engine Google. But its own internet business, AOL, had has mixed fortunes. It lost 464,000 subscribers in the fourth quarter profits were lower than in the preceding three quarters. However, the company said AOL's underlying profit before exceptional items rose 8% on the back of stronger internet advertising revenues. It hopes to increase subscribers by offering the online service free to TimeWarner internet customers and will try to sign up AOL's existing customers for high-speed broadband. TimeWarner also has to restate 2000 and 2003 results following a probe by the US Securities Exchange Commission (SEC), which is close to concluding.\n\nTime Warner's fourth quarter profits were slightly better than analysts' expectations. But its film division saw profits slump 27% to $284m, helped by box-office flops Alexander and Catwoman, a sharp contrast to year-earlier, when the third and final film in the Lord of the Rings trilogy boosted results. For the full-year, TimeWarner posted a profit of $3.36bn, up 27% from its 2003 performance, while revenues grew 6.4% to $42.09bn. \"Our financial performance was strong, meeting or exceeding all of our full-year objectives and greatly enhancing our flexibility,\" chairman and chief executive Richard Parsons said. For 2005, TimeWarner is projecting operating earnings growth of around 5%, and also expects higher revenue and wider profit margins.\n\nTimeWarner is to restate its accounts as part of efforts to resolve an inquiry into AOL by US market regulators. It has already offered to pay $300m to settle charges, in a deal that is under review by the SEC. The company said it was unable to estimate the amount it needed to set aside for legal reserves, which it previously set at $500m. It intends to adjust the way it accounts for a deal with German music publisher Bertelsmann's purchase of a stake in AOL Europe, which it had reported as advertising revenue. It will now book the sale of its stake in AOL Europe as a loss on the value of that stake.\n\n"
        }
       ]
      }
     },
     "363d527b513e4ffab5f46f0a9cc2737b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "3a292d4848e340bbb03efcb44b4c5546": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntSliderModel",
      "state": {
       "description": "doc_id",
       "layout": "IPY_MODEL_9ed5f929f05949ca81047f3a84ae1e84",
       "max": 2224,
       "style": "IPY_MODEL_5c718fb3a6c54077b79a5a284570c0e0"
      }
     },
     "4a3ed32352254eeebe3368a117a9a382": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "SliderStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "4df391293247441dba818e14d38af8ea": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "510ec16b4b0c460395beef57d1b19385": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5259f03c2a4f4730902ea770b0867c84": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_1f898984afc444e49b13dc17023418de",
       "outputs": [
        {
         "name": "stdout",
         "output_type": "stream",
         "text": "{'filename': 'data/bbc/business/001.txt'}\n\nad sales boost time warner profit quarterly profits at us media giant timewarner jumped to $1.13bn â£600m for the three months to december from $639m year-earlier the firm which is now one of the biggest investors in google benefited from sales of high-speed internet connections and higher advert sales timewarner said fourth quarter sales rose to $11.1bn from $10.9bn its profits were buoyed by one-off gains which offset profit dip at warner bros and less users for aol time warner said on friday that it now owns of search-engine google but its own internet business aol had has mixed fortunes it lost subscribers in the fourth quarter profits were lower than in the preceding three quarters however the company said aols underlying profit before exceptional items rose on the back of stronger internet advertising revenues it hopes to increase subscribers by offering the online service free to timewarner internet customers and will try to sign up aols existing customers for high-speed broadband timewarner also has to restate and results following probe by the us securities exchange commission sec which is close to concluding time warners fourth quarter profits were slightly better than analysts expectations but its film division saw profits slump to $284m helped by box-office flops alexander and catwoman sharp contrast to year-earlier when the third and final film in the lord_of_the_rings trilogy boosted results for the full-year timewarner posted profit of $3.36bn up from its performance while revenues grew to $42.09bn our financial performance was strong meeting or exceeding all of our full-year objectives and greatly enhancing our flexibility chairman and chief_executive richard parsons said for timewarner is projecting operating earnings growth of around and also expects higher revenue and wider profit margins timewarner is to restate its accounts as part of efforts to resolve an inquiry into aol by us market regulators it has already offered to pay $300m to settle charges in deal that is under review by the sec the company said it was unable to estimate the amount it needed to set aside for legal reserves which it previously set at $500m it intends to adjust the way it accounts for deal with german music publisher bertelsmanns purchase of stake in aol europe which it had reported as advertising revenue it will now book the sale of its stake in aol europe as loss on the value of that stake\n"
        }
       ]
      }
     },
     "596c9f0a5ee54976b5c305733046eb20": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "VBoxModel",
      "state": {
       "_dom_classes": [
        "widget-interact"
       ],
       "children": [
        "IPY_MODEL_72c64a752960491d904da0ddbdf4798b",
        "IPY_MODEL_972d09b6a6024423a6de54b5f13114a5"
       ],
       "layout": "IPY_MODEL_c8473dedaa18462dbd9ed2df63cffc33"
      }
     },
     "5c718fb3a6c54077b79a5a284570c0e0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "SliderStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "6e2d7a2b5ae14a82bc6493af5230b988": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "72c64a752960491d904da0ddbdf4798b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntSliderModel",
      "state": {
       "description": "doc_id",
       "layout": "IPY_MODEL_4df391293247441dba818e14d38af8ea",
       "max": 2224,
       "style": "IPY_MODEL_4a3ed32352254eeebe3368a117a9a382"
      }
     },
     "9636b5d65c6c45df80d319d5ae4775a8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntSliderModel",
      "state": {
       "description": "doc_id",
       "layout": "IPY_MODEL_13eb7efb1d294d8390b86540aaaeeb07",
       "max": 2224,
       "style": "IPY_MODEL_0176bce07868479ba1260626d7e96ae2"
      }
     },
     "963c8fb4d7e24e14b38a4ef5f44001e5": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_c7609d692cbd4e8e967cb8cbd2357b69",
       "outputs": [
        {
         "name": "stdout",
         "output_type": "stream",
         "text": "{'filename': 'data/bbc/business/001.txt'}\n\nad sales boost time warner profit quarterly profits at us media giant timewarner jumped 76% to $1.13bn â£600m for the three months to december from $639m year-earlier the firm which is now one of the biggest investors in google benefited from sales of high-speed internet connections and higher advert sales timewarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn its profits were buoyed by one-off gains which offset a profit dip at warner bros and less users for aol time warner said on friday that it now owns 8% of search-engine google but its own internet business aol had has mixed fortunes it lost 464,000 subscribers in the fourth quarter profits were lower than in the preceding three quarters however the company said aols underlying profit before exceptional items rose 8% on the back of stronger internet advertising revenues it hopes to increase subscribers by offering the online service free to timewarner internet customers and will try to sign up aols existing customers for high-speed broadband timewarner also has to restate 2000 and 2003 results following a probe by the us securities exchange commission sec which is close to concluding time warners fourth quarter profits were slightly better than analysts expectations but its film division saw profits slump 27% to $284m helped by box-office flops alexander and catwoman a sharp contrast to year-earlier when the third and final film in the lord_of_the_rings trilogy boosted results for the full-year timewarner posted a profit of $3.36bn up 27% from its 2003 performance while revenues grew 6.4% to $42.09bn our financial performance was strong meeting or exceeding all of our full-year objectives and greatly enhancing our flexibility chairman and chief_executive richard parsons said for 2005 timewarner is projecting operating earnings growth of around 5% and also expects higher revenue and wider profit margins timewarner is to restate its accounts as part of efforts to resolve an inquiry into aol by us market regulators it has already offered to pay $300m to settle charges in a deal that is under review by the sec the company said it was unable to estimate the amount it needed to set aside for legal reserves which it previously set at $500m it intends to adjust the way it accounts for a deal with german music publisher bertelsmanns purchase of a stake in aol europe which it had reported as advertising revenue it will now book the sale of its stake in aol europe as a loss on the value of that stake\n"
        }
       ]
      }
     },
     "972d09b6a6024423a6de54b5f13114a5": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_6e2d7a2b5ae14a82bc6493af5230b988",
       "outputs": [
        {
         "name": "stdout",
         "output_type": "stream",
         "text": "{'filename': 'data/bbc/business/001.txt'}\n\nad sales boost time warner profit quarterly profits at us media giant timewarner jumped 76% to $1.13bn â£600m for the three months to december from $639m year-earlier the firm which is now one of the biggest investors in google benefited from sales of high-speed internet connections and higher advert sales timewarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn its profits were buoyed by one-off gains which offset a profit dip at warner bros and less users for aol time warner said on friday that it now owns 8% of search-engine google but its own internet business aol had has mixed fortunes it lost 464,000 subscribers in the fourth quarter profits were lower than in the preceding three quarters however the company said aols underlying profit before exceptional items rose 8% on the back of stronger internet advertising revenues it hopes to increase subscribers by offering the online service free to timewarner internet customers and will try to sign up aols existing customers for high-speed broadband timewarner also has to restate 2000 and 2003 results following a probe by the us securities exchange commission sec which is close to concluding time warners fourth quarter profits were slightly better than analysts expectations but its film division saw profits slump 27% to $284m helped by box-office flops alexander and catwoman a sharp contrast to year-earlier when the third and final film in the lord of the rings trilogy boosted results for the full-year timewarner posted a profit of $3.36bn up 27% from its 2003 performance while revenues grew 6.4% to $42.09bn our financial performance was strong meeting or exceeding all of our full-year objectives and greatly enhancing our flexibility chairman and chief executive richard parsons said for 2005 timewarner is projecting operating earnings growth of around 5% and also expects higher revenue and wider profit margins timewarner is to restate its accounts as part of efforts to resolve an inquiry into aol by us market regulators it has already offered to pay $300m to settle charges in a deal that is under review by the sec the company said it was unable to estimate the amount it needed to set aside for legal reserves which it previously set at $500m it intends to adjust the way it accounts for a deal with german music publisher bertelsmanns purchase of a stake in aol europe which it had reported as advertising revenue it will now book the sale of its stake in aol europe as a loss on the value of that stake\n"
        }
       ]
      }
     },
     "9ed5f929f05949ca81047f3a84ae1e84": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b5cc154c0da84a9b95e463fe55e0d966": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "bc3c0ab7399b4b16b5d34f7b6307998b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "VBoxModel",
      "state": {
       "_dom_classes": [
        "widget-interact"
       ],
       "children": [
        "IPY_MODEL_e4e663e13b1c4bb4a38edf7e816b4937",
        "IPY_MODEL_300ddb0791ce4396893229818dff50e8"
       ],
       "layout": "IPY_MODEL_363d527b513e4ffab5f46f0a9cc2737b"
      }
     },
     "c5cdae633e8d419f8e138445d94e8bbc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "SliderStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "c7609d692cbd4e8e967cb8cbd2357b69": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c8473dedaa18462dbd9ed2df63cffc33": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e4e663e13b1c4bb4a38edf7e816b4937": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntSliderModel",
      "state": {
       "description": "doc_id",
       "layout": "IPY_MODEL_0f6858aff9a6418893b5db2b75af667d",
       "max": 2224,
       "style": "IPY_MODEL_c5cdae633e8d419f8e138445d94e8bbc"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
