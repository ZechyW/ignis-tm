<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>ignis.util.improved_phraser API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import math
import time
import uuid

from tqdm.auto import tqdm

from ignis.util.lazy_loader import LazyLoader


class ImprovedPhraser:
    &#34;&#34;&#34;
    A slightly different implementation of automated n-gram detection derived from
    Gensim&#39;s Phraser

    - Applies to highest-scoring and longest n-grams in a document first
    - Chained n-grams that have the same score are combined into a single n-gram
      (e.g., if &#34;quick brown&#34; and &#34;brown fox&#34; have the same score, a new n-gram
      &#34;quick brown fox&#34; is created with that score)

    Assumes that its input is an iterable of strings (Gensim&#39;s phraser model works
    with bytes by default)

    Parameters
    ----------
    sentences: iterable of iterable of str
        Required argument that will be passed to `gensim.models.Phrases` to generate
        the base n-gram model
    min_count, threshold, max_vocab_size, scoring, common_terms: optional
        Optional arguments that will be passed directly to `gensim.models.Phrases` to
        generate the base n-gram model
    delimiter: str, optional
        Delimiter to join detected phrases with. Will not actually be passed to the
        underlying Gensim model (which expects bytes and not a string anyway).
    drop_non_alpha: bool, optional
        Whether or not to include phrases that consist entirely of non-alphabetic
        strings. Will drop them by default.
    verbose: bool, optional
    &#34;&#34;&#34;

    def __init__(
        self, sentences, delimiter=&#34; &#34;, drop_non_alpha=True, verbose=False, **kwargs,
    ):
        start_time = time.perf_counter()

        self.delimiter = delimiter

        gensim = LazyLoader(&#34;gensim&#34;)

        gensim_kwarg_names = [
            &#34;min_count&#34;,
            &#34;threshold&#34;,
            &#34;max_vocab_size&#34;,
            &#34;scoring&#34;,
            &#34;common_terms&#34;,
        ]
        gensim_kwargs = {
            key: value
            for key, value in kwargs.items()
            if key in gensim_kwarg_names and value is not None
        }

        # Pass a bogus delimiter to gensim -- The default of b&#34;_&#34; will throw off the
        # phrase scores if any terms already have underscores in them
        gensim_kwargs[&#34;delimiter&#34;] = b&#34;&lt;*ignis*&gt;&#34;

        model = gensim.models.Phrases(sentences=sentences, **gensim_kwargs)
        model = gensim.models.phrases.Phraser(model)
        model = model.phrasegrams.items()

        elapsed = time.perf_counter() - start_time
        start_time = time.perf_counter()
        if verbose:
            print(f&#34;Gensim Phraser initialised. {elapsed:.3f}s&#34;, flush=True)

        # model is a list of tuples(&lt;n-gram&gt;, &lt;score&gt;), where &lt;n-gram&gt; is a
        # tuple (&lt;token 1&gt;, &lt;token 2&gt;, ...).
        # N.B.: Gensim treats all the tokens as bytes, so we will need to .decode()
        # to strings when using them below.

        # The final list of sorted phrases
        self.phrases = []

        # Sort by scores then length, using set() to deduplicate
        scores = set(score for _, score in model)
        scores = sorted(list(scores), reverse=True)
        for score_tier in scores:
            score_phrases = set(
                phrase for phrase, score in model if score == score_tier
            )
            # Decode the byte tokens in the phrase tuple
            score_phrases = set(
                tuple(token.decode() for token in phrase) for phrase in score_phrases
            )

            # Recursively expanded phrase list (including &#34;chained&#34; phrases),
            # again using set() for deduplication
            merged = set()
            done = False
            while not done:
                # Start by assuming we are done; we will reset the flag if anything
                # shows up on this round
                done = True

                # For each original n-gram from the Phraser...
                for phrase in score_phrases:
                    # Check if it is completely non-alphabetic
                    if drop_non_alpha and not any(
                        [token.isalpha() for token in phrase]
                    ):
                        continue

                    # And ensure that the original phrase itself is in the final
                    # result set (assuming it passes the `drop_non_alpha` setting)
                    merged.add(phrase)

                    # Look for chains with original phrases or already-merged extras
                    others = (score_phrases | merged) - {phrase}

                    for other in others:
                        # Sanity checks (prevents infinite merging loops)
                        str_phrase = &#34; &#34;.join(phrase)
                        str_other = &#34; &#34;.join(other)
                        if str_phrase in str_other or str_other in str_phrase:
                            continue
                        if (
                            len(phrase) == 2
                            and len(other) == 2
                            and phrase[0] == other[1]
                            and phrase[1] == other[0]
                        ):
                            continue

                        # Non-alphabetic check
                        if drop_non_alpha and not any(
                            [token.isalpha() for token in other]
                        ):
                            continue

                        new_phrase = None
                        if phrase[0] == other[-1]:
                            new_phrase = other + phrase[1:]
                        if phrase[-1] == other[0]:
                            new_phrase = phrase + other[1:]

                        if new_phrase and new_phrase not in merged:
                            merged.add(new_phrase)
                            done = False

            # Sort by tokens alphabetically, then by length
            sorted_merged = sorted(list(merged), key=lambda x: &#34; &#34;.join(x))
            sorted_merged = sorted(sorted_merged, key=lambda x: len(x), reverse=True)
            self.phrases += [(list(phrase), score_tier) for phrase in sorted_merged]

        # Organise the new list of phrases for subsequent application
        self.by_first = {}
        self._map_by_first_token()

        # And done -- Ready to phrase.
        elapsed = time.perf_counter() - start_time
        if verbose:
            print(f&#34;Improved Phraser initialised. {elapsed:.3f}s&#34;, flush=True)

    def _map_by_first_token(self):
        &#34;&#34;&#34;
        Maps this instance&#39;s phrases to a Dictionary by first token for more
        efficient phrasing performance.

        (N.B.: Assumes `self.phrases` is already sorted by score then length)
        &#34;&#34;&#34;
        for phrase, score in self.phrases:
            head = phrase[0]
            if head not in self.by_first:
                self.by_first[head] = []
            self.by_first[head].append((phrase, score))

    def find_ngrams(self, docs, threshold=-math.inf, verbose=False):
        &#34;&#34;&#34;
        Perform n-gram replacement on the given documents using the phrase model
        trained for this instance.

        Detects significant bigrams by default, but bigrams with the same phrase
        score are merged, and common terms are ignored (as described in the class
        documentation).

        To detect the next higher order of n-grams (viz., trigrams and above),
        you will need to create a new ImprovedPhraser instance, passing the results
        of this function as the new set of base sentences to use.

        Parameters
        ----------
        docs: iterable of iterable of str
            Each doc in docs should be an iterable of strings -- Substring matching
            will be used in the replacement process
        threshold: float, optional
            Optionally set a new phrasing threshold; if not set, will apply all the
            available phrases (which are determined by the value of `threshold`
            passed to the base gensim model on init)
        verbose: bool, optional

        Returns
        -------
        iterable of iterable of str
        &#34;&#34;&#34;
        new_docs = []

        if verbose:
            docs = tqdm(docs)

        for doc in docs:
            # Find all candidate phrase chains originating from each token,
            # then merge the highest-scoring one.
            new_doc = doc[:]
            index = 0
            while index &lt; len(new_doc):
                token = new_doc[index]
                best_candidate = self._find_best_candidate(
                    token, index, new_doc, threshold
                )

                # No phrases starting on this token
                if best_candidate is None:
                    index += 1
                    continue

                # Preemptively merge any candidates found in the chain starting from
                # this token
                exhausted_chain = False
                while best_candidate[&#34;start_index&#34;] &gt; index:
                    # Merge
                    new_doc[
                        best_candidate[&#34;start_index&#34;] : best_candidate[&#34;start_index&#34;]
                        + len(best_candidate[&#34;phrase&#34;])
                    ] = [self.delimiter.join(best_candidate[&#34;phrase&#34;])]

                    # Get the next highest scoring candidate in the chain
                    best_candidate = self._find_best_candidate(
                        token, index, new_doc, threshold
                    )

                    if best_candidate is None:
                        # There is no candidate for merging anywhere in the chain,
                        # including directly from the current token...
                        exhausted_chain = True
                        break

                if exhausted_chain:
                    # ... so continue with the next token in the document
                    index += 1
                    continue

                # Still here? Then there is a best candidate phrase for merging,
                # and it starts on this token
                assert best_candidate[&#34;start_index&#34;] == index
                new_doc[index : index + len(best_candidate[&#34;phrase&#34;])] = [
                    self.delimiter.join(best_candidate[&#34;phrase&#34;])
                ]
                index += 1

            new_docs.append(new_doc)

        return new_docs

    def _find_best_candidate(self, start_token, start_index, document, threshold):
        &#34;&#34;&#34;
        Examines all the candidate phrase chains originating from the given token in
        this Phraser model and returns the highest-scoring one.

        E.g., if the document is [&#34;likes&#34;, &#34;swimming&#34;, &#34;pool&#34;, &#34;tubes&#34;] and &#34;swimming
        pool&#34; has a higher phrase score than the other two bigrams, it will be given
        priority even though it is in the middle of the chain (i.e., the phraser
        intentionally skips merging the lower-scoring &#34;likes swimming&#34;).

        Parameters
        ----------
        start_token
        start_index
        document
        threshold

        Returns
        -------
        dict with keys &#34;start_index&#34;, &#34;phrase&#34;, and &#34;score&#34;; or
        None if `start_token` is not in the model&#39;s phrases
        &#34;&#34;&#34;
        if start_token not in self.by_first:
            return None

        # Maps next `start_token` -&gt; `start_index` entries for recursion
        next_starts = {}

        best_candidate = None
        phrases = self.by_first[start_token]

        for phrase, score in phrases:
            if score &lt; threshold:
                continue

            if document[start_index : start_index + len(phrase)] == phrase:
                # Found a candidate chain
                this_candidate = dict(
                    start_index=start_index, phrase=phrase, score=score
                )
            else:
                # This phrase does not match the document
                continue

            # Set new `best_candidate` if appropriate
            if best_candidate is None:
                best_candidate = this_candidate
            else:
                if this_candidate[&#34;score&#34;] &gt; best_candidate[&#34;score&#34;] or (
                    this_candidate[&#34;score&#34;] == best_candidate[&#34;score&#34;]
                    and len(this_candidate[&#34;phrase&#34;]) &gt; len(best_candidate[&#34;phrase&#34;])
                ):
                    best_candidate = this_candidate

            # If we are still in this loop, this phrase is valid, and could form a
            # chain with the next few tokens in the document; queue them for
            # recursive checking
            for index, next_start in enumerate(phrase[1:]):
                if (
                    document[start_index + 1 + index] == next_start
                    and next_start not in next_starts
                ):
                    next_starts[next_start] = start_index + 1 + index

        # Cash out the chains in `next_starts`
        for next_start, next_index in next_starts.items():
            next_candidate = self._find_best_candidate(
                next_start, next_index, document, threshold
            )
            if next_candidate is None:
                continue

            if next_candidate[&#34;score&#34;] &gt; best_candidate[&#34;score&#34;] or (
                next_candidate[&#34;score&#34;] == best_candidate[&#34;score&#34;]
                and len(next_candidate[&#34;phrase&#34;]) &gt; len(best_candidate[&#34;phrase&#34;])
            ):
                best_candidate = next_candidate

        return best_candidate

    def find_ngrams_str(self, docs, threshold=-math.inf):
        &#34;&#34;&#34;
        Perform n-gram replacement on the given documents using the phrase model
        trained for this instance.

        Version that uses string replacement: Much slower and slightly divergent from
        the default algorithm, which always ensures that the right-most relevant
        n-gram is merged first.

        Parameters
        ----------
        docs: iterable of iterable of str
            Each doc in docs should be an iterable of strings -- Substring matching
            will be used in the replacement process
        threshold: float, optional
            Optionally set a new phrasing threshold; if not set, will apply all the
            available phrases (which are determined by the value of `threshold`
            passed to the base gensim model on init)

        Returns
        -------
        iterable of iterable of str
        &#34;&#34;&#34;
        # A &#34;safe&#34; delimiter that will be used for the intermediate string joins;
        # should be guaranteed to be different from `self.delimiter` so that we can
        # perform recursive phrasing without inadvertently re-splitting previously
        # joined tokens
        search_delimiter = f&#34;&lt;*&gt;&#34;

        new_docs = []
        for doc in docs:
            # In case we really need a new delimiter: Use part of a UUID (not the
            # whole, to conserve memory)
            while search_delimiter in doc or search_delimiter == self.delimiter:
                search_delimiter = f&#34;&lt;{str(uuid.uuid4())[:3]}&gt;&#34;

            str_doc = search_delimiter.join(doc)

            for phrase, score in self.phrases:
                if score &lt; threshold or phrase[0] not in doc:
                    continue

                search_phrase = search_delimiter.join(phrase)
                replace_phrase = self.delimiter.join(phrase)

                str_phrase = f&#34;{search_delimiter}{search_phrase}{search_delimiter}&#34;
                str_replace = f&#34;{search_delimiter}{replace_phrase}{search_delimiter}&#34;
                str_doc = str_doc.replace(str_phrase, str_replace)

                # Handle matches at start/end of lines
                start_phrase = f&#34;{search_phrase}{search_delimiter}&#34;
                start_replace = f&#34;{replace_phrase}{search_delimiter}&#34;
                end_phrase = f&#34;{search_delimiter}{search_phrase}&#34;
                end_replace = f&#34;{search_delimiter}{replace_phrase}&#34;

                if str_doc.startswith(start_phrase):
                    str_doc = start_replace + str_doc[len(start_phrase) :]
                if str_doc.endswith(end_phrase):
                    str_doc = str_doc[0 : -len(end_phrase)] + end_replace

            new_docs.append(str_doc.split(search_delimiter))

        return new_docs</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="ignis.util.improved_phraser.ImprovedPhraser"><code class="flex name class">
<span>class <span class="ident">ImprovedPhraser</span></span>
<span>(</span><span>sentences, delimiter=' ', drop_non_alpha=True, verbose=False, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>A slightly different implementation of automated n-gram detection derived from
Gensim's Phraser</p>
<ul>
<li>Applies to highest-scoring and longest n-grams in a document first</li>
<li>Chained n-grams that have the same score are combined into a single n-gram
(e.g., if "quick brown" and "brown fox" have the same score, a new n-gram
"quick brown fox" is created with that score)</li>
</ul>
<p>Assumes that its input is an iterable of strings (Gensim's phraser model works
with bytes by default)</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>sentences</code></strong> :&ensp;<code>iterable</code> of <code>iterable</code> of <code>str</code></dt>
<dd>Required argument that will be passed to <code>gensim.models.Phrases</code> to generate
the base n-gram model</dd>
<dt><strong><code>min_count</code></strong>, <strong><code>threshold</code></strong>, <strong><code>max_vocab_size</code></strong>, <strong><code>scoring</code></strong>, <strong><code>common_terms</code></strong> :&ensp;<code>optional</code></dt>
<dd>Optional arguments that will be passed directly to <code>gensim.models.Phrases</code> to
generate the base n-gram model</dd>
<dt><strong><code>delimiter</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Delimiter to join detected phrases with. Will not actually be passed to the
underlying Gensim model (which expects bytes and not a string anyway).</dd>
<dt><strong><code>drop_non_alpha</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether or not to include phrases that consist entirely of non-alphabetic
strings. Will drop them by default.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ImprovedPhraser:
    &#34;&#34;&#34;
    A slightly different implementation of automated n-gram detection derived from
    Gensim&#39;s Phraser

    - Applies to highest-scoring and longest n-grams in a document first
    - Chained n-grams that have the same score are combined into a single n-gram
      (e.g., if &#34;quick brown&#34; and &#34;brown fox&#34; have the same score, a new n-gram
      &#34;quick brown fox&#34; is created with that score)

    Assumes that its input is an iterable of strings (Gensim&#39;s phraser model works
    with bytes by default)

    Parameters
    ----------
    sentences: iterable of iterable of str
        Required argument that will be passed to `gensim.models.Phrases` to generate
        the base n-gram model
    min_count, threshold, max_vocab_size, scoring, common_terms: optional
        Optional arguments that will be passed directly to `gensim.models.Phrases` to
        generate the base n-gram model
    delimiter: str, optional
        Delimiter to join detected phrases with. Will not actually be passed to the
        underlying Gensim model (which expects bytes and not a string anyway).
    drop_non_alpha: bool, optional
        Whether or not to include phrases that consist entirely of non-alphabetic
        strings. Will drop them by default.
    verbose: bool, optional
    &#34;&#34;&#34;

    def __init__(
        self, sentences, delimiter=&#34; &#34;, drop_non_alpha=True, verbose=False, **kwargs,
    ):
        start_time = time.perf_counter()

        self.delimiter = delimiter

        gensim = LazyLoader(&#34;gensim&#34;)

        gensim_kwarg_names = [
            &#34;min_count&#34;,
            &#34;threshold&#34;,
            &#34;max_vocab_size&#34;,
            &#34;scoring&#34;,
            &#34;common_terms&#34;,
        ]
        gensim_kwargs = {
            key: value
            for key, value in kwargs.items()
            if key in gensim_kwarg_names and value is not None
        }

        # Pass a bogus delimiter to gensim -- The default of b&#34;_&#34; will throw off the
        # phrase scores if any terms already have underscores in them
        gensim_kwargs[&#34;delimiter&#34;] = b&#34;&lt;*ignis*&gt;&#34;

        model = gensim.models.Phrases(sentences=sentences, **gensim_kwargs)
        model = gensim.models.phrases.Phraser(model)
        model = model.phrasegrams.items()

        elapsed = time.perf_counter() - start_time
        start_time = time.perf_counter()
        if verbose:
            print(f&#34;Gensim Phraser initialised. {elapsed:.3f}s&#34;, flush=True)

        # model is a list of tuples(&lt;n-gram&gt;, &lt;score&gt;), where &lt;n-gram&gt; is a
        # tuple (&lt;token 1&gt;, &lt;token 2&gt;, ...).
        # N.B.: Gensim treats all the tokens as bytes, so we will need to .decode()
        # to strings when using them below.

        # The final list of sorted phrases
        self.phrases = []

        # Sort by scores then length, using set() to deduplicate
        scores = set(score for _, score in model)
        scores = sorted(list(scores), reverse=True)
        for score_tier in scores:
            score_phrases = set(
                phrase for phrase, score in model if score == score_tier
            )
            # Decode the byte tokens in the phrase tuple
            score_phrases = set(
                tuple(token.decode() for token in phrase) for phrase in score_phrases
            )

            # Recursively expanded phrase list (including &#34;chained&#34; phrases),
            # again using set() for deduplication
            merged = set()
            done = False
            while not done:
                # Start by assuming we are done; we will reset the flag if anything
                # shows up on this round
                done = True

                # For each original n-gram from the Phraser...
                for phrase in score_phrases:
                    # Check if it is completely non-alphabetic
                    if drop_non_alpha and not any(
                        [token.isalpha() for token in phrase]
                    ):
                        continue

                    # And ensure that the original phrase itself is in the final
                    # result set (assuming it passes the `drop_non_alpha` setting)
                    merged.add(phrase)

                    # Look for chains with original phrases or already-merged extras
                    others = (score_phrases | merged) - {phrase}

                    for other in others:
                        # Sanity checks (prevents infinite merging loops)
                        str_phrase = &#34; &#34;.join(phrase)
                        str_other = &#34; &#34;.join(other)
                        if str_phrase in str_other or str_other in str_phrase:
                            continue
                        if (
                            len(phrase) == 2
                            and len(other) == 2
                            and phrase[0] == other[1]
                            and phrase[1] == other[0]
                        ):
                            continue

                        # Non-alphabetic check
                        if drop_non_alpha and not any(
                            [token.isalpha() for token in other]
                        ):
                            continue

                        new_phrase = None
                        if phrase[0] == other[-1]:
                            new_phrase = other + phrase[1:]
                        if phrase[-1] == other[0]:
                            new_phrase = phrase + other[1:]

                        if new_phrase and new_phrase not in merged:
                            merged.add(new_phrase)
                            done = False

            # Sort by tokens alphabetically, then by length
            sorted_merged = sorted(list(merged), key=lambda x: &#34; &#34;.join(x))
            sorted_merged = sorted(sorted_merged, key=lambda x: len(x), reverse=True)
            self.phrases += [(list(phrase), score_tier) for phrase in sorted_merged]

        # Organise the new list of phrases for subsequent application
        self.by_first = {}
        self._map_by_first_token()

        # And done -- Ready to phrase.
        elapsed = time.perf_counter() - start_time
        if verbose:
            print(f&#34;Improved Phraser initialised. {elapsed:.3f}s&#34;, flush=True)

    def _map_by_first_token(self):
        &#34;&#34;&#34;
        Maps this instance&#39;s phrases to a Dictionary by first token for more
        efficient phrasing performance.

        (N.B.: Assumes `self.phrases` is already sorted by score then length)
        &#34;&#34;&#34;
        for phrase, score in self.phrases:
            head = phrase[0]
            if head not in self.by_first:
                self.by_first[head] = []
            self.by_first[head].append((phrase, score))

    def find_ngrams(self, docs, threshold=-math.inf, verbose=False):
        &#34;&#34;&#34;
        Perform n-gram replacement on the given documents using the phrase model
        trained for this instance.

        Detects significant bigrams by default, but bigrams with the same phrase
        score are merged, and common terms are ignored (as described in the class
        documentation).

        To detect the next higher order of n-grams (viz., trigrams and above),
        you will need to create a new ImprovedPhraser instance, passing the results
        of this function as the new set of base sentences to use.

        Parameters
        ----------
        docs: iterable of iterable of str
            Each doc in docs should be an iterable of strings -- Substring matching
            will be used in the replacement process
        threshold: float, optional
            Optionally set a new phrasing threshold; if not set, will apply all the
            available phrases (which are determined by the value of `threshold`
            passed to the base gensim model on init)
        verbose: bool, optional

        Returns
        -------
        iterable of iterable of str
        &#34;&#34;&#34;
        new_docs = []

        if verbose:
            docs = tqdm(docs)

        for doc in docs:
            # Find all candidate phrase chains originating from each token,
            # then merge the highest-scoring one.
            new_doc = doc[:]
            index = 0
            while index &lt; len(new_doc):
                token = new_doc[index]
                best_candidate = self._find_best_candidate(
                    token, index, new_doc, threshold
                )

                # No phrases starting on this token
                if best_candidate is None:
                    index += 1
                    continue

                # Preemptively merge any candidates found in the chain starting from
                # this token
                exhausted_chain = False
                while best_candidate[&#34;start_index&#34;] &gt; index:
                    # Merge
                    new_doc[
                        best_candidate[&#34;start_index&#34;] : best_candidate[&#34;start_index&#34;]
                        + len(best_candidate[&#34;phrase&#34;])
                    ] = [self.delimiter.join(best_candidate[&#34;phrase&#34;])]

                    # Get the next highest scoring candidate in the chain
                    best_candidate = self._find_best_candidate(
                        token, index, new_doc, threshold
                    )

                    if best_candidate is None:
                        # There is no candidate for merging anywhere in the chain,
                        # including directly from the current token...
                        exhausted_chain = True
                        break

                if exhausted_chain:
                    # ... so continue with the next token in the document
                    index += 1
                    continue

                # Still here? Then there is a best candidate phrase for merging,
                # and it starts on this token
                assert best_candidate[&#34;start_index&#34;] == index
                new_doc[index : index + len(best_candidate[&#34;phrase&#34;])] = [
                    self.delimiter.join(best_candidate[&#34;phrase&#34;])
                ]
                index += 1

            new_docs.append(new_doc)

        return new_docs

    def _find_best_candidate(self, start_token, start_index, document, threshold):
        &#34;&#34;&#34;
        Examines all the candidate phrase chains originating from the given token in
        this Phraser model and returns the highest-scoring one.

        E.g., if the document is [&#34;likes&#34;, &#34;swimming&#34;, &#34;pool&#34;, &#34;tubes&#34;] and &#34;swimming
        pool&#34; has a higher phrase score than the other two bigrams, it will be given
        priority even though it is in the middle of the chain (i.e., the phraser
        intentionally skips merging the lower-scoring &#34;likes swimming&#34;).

        Parameters
        ----------
        start_token
        start_index
        document
        threshold

        Returns
        -------
        dict with keys &#34;start_index&#34;, &#34;phrase&#34;, and &#34;score&#34;; or
        None if `start_token` is not in the model&#39;s phrases
        &#34;&#34;&#34;
        if start_token not in self.by_first:
            return None

        # Maps next `start_token` -&gt; `start_index` entries for recursion
        next_starts = {}

        best_candidate = None
        phrases = self.by_first[start_token]

        for phrase, score in phrases:
            if score &lt; threshold:
                continue

            if document[start_index : start_index + len(phrase)] == phrase:
                # Found a candidate chain
                this_candidate = dict(
                    start_index=start_index, phrase=phrase, score=score
                )
            else:
                # This phrase does not match the document
                continue

            # Set new `best_candidate` if appropriate
            if best_candidate is None:
                best_candidate = this_candidate
            else:
                if this_candidate[&#34;score&#34;] &gt; best_candidate[&#34;score&#34;] or (
                    this_candidate[&#34;score&#34;] == best_candidate[&#34;score&#34;]
                    and len(this_candidate[&#34;phrase&#34;]) &gt; len(best_candidate[&#34;phrase&#34;])
                ):
                    best_candidate = this_candidate

            # If we are still in this loop, this phrase is valid, and could form a
            # chain with the next few tokens in the document; queue them for
            # recursive checking
            for index, next_start in enumerate(phrase[1:]):
                if (
                    document[start_index + 1 + index] == next_start
                    and next_start not in next_starts
                ):
                    next_starts[next_start] = start_index + 1 + index

        # Cash out the chains in `next_starts`
        for next_start, next_index in next_starts.items():
            next_candidate = self._find_best_candidate(
                next_start, next_index, document, threshold
            )
            if next_candidate is None:
                continue

            if next_candidate[&#34;score&#34;] &gt; best_candidate[&#34;score&#34;] or (
                next_candidate[&#34;score&#34;] == best_candidate[&#34;score&#34;]
                and len(next_candidate[&#34;phrase&#34;]) &gt; len(best_candidate[&#34;phrase&#34;])
            ):
                best_candidate = next_candidate

        return best_candidate

    def find_ngrams_str(self, docs, threshold=-math.inf):
        &#34;&#34;&#34;
        Perform n-gram replacement on the given documents using the phrase model
        trained for this instance.

        Version that uses string replacement: Much slower and slightly divergent from
        the default algorithm, which always ensures that the right-most relevant
        n-gram is merged first.

        Parameters
        ----------
        docs: iterable of iterable of str
            Each doc in docs should be an iterable of strings -- Substring matching
            will be used in the replacement process
        threshold: float, optional
            Optionally set a new phrasing threshold; if not set, will apply all the
            available phrases (which are determined by the value of `threshold`
            passed to the base gensim model on init)

        Returns
        -------
        iterable of iterable of str
        &#34;&#34;&#34;
        # A &#34;safe&#34; delimiter that will be used for the intermediate string joins;
        # should be guaranteed to be different from `self.delimiter` so that we can
        # perform recursive phrasing without inadvertently re-splitting previously
        # joined tokens
        search_delimiter = f&#34;&lt;*&gt;&#34;

        new_docs = []
        for doc in docs:
            # In case we really need a new delimiter: Use part of a UUID (not the
            # whole, to conserve memory)
            while search_delimiter in doc or search_delimiter == self.delimiter:
                search_delimiter = f&#34;&lt;{str(uuid.uuid4())[:3]}&gt;&#34;

            str_doc = search_delimiter.join(doc)

            for phrase, score in self.phrases:
                if score &lt; threshold or phrase[0] not in doc:
                    continue

                search_phrase = search_delimiter.join(phrase)
                replace_phrase = self.delimiter.join(phrase)

                str_phrase = f&#34;{search_delimiter}{search_phrase}{search_delimiter}&#34;
                str_replace = f&#34;{search_delimiter}{replace_phrase}{search_delimiter}&#34;
                str_doc = str_doc.replace(str_phrase, str_replace)

                # Handle matches at start/end of lines
                start_phrase = f&#34;{search_phrase}{search_delimiter}&#34;
                start_replace = f&#34;{replace_phrase}{search_delimiter}&#34;
                end_phrase = f&#34;{search_delimiter}{search_phrase}&#34;
                end_replace = f&#34;{search_delimiter}{replace_phrase}&#34;

                if str_doc.startswith(start_phrase):
                    str_doc = start_replace + str_doc[len(start_phrase) :]
                if str_doc.endswith(end_phrase):
                    str_doc = str_doc[0 : -len(end_phrase)] + end_replace

            new_docs.append(str_doc.split(search_delimiter))

        return new_docs</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="ignis.util.improved_phraser.ImprovedPhraser.find_ngrams"><code class="name flex">
<span>def <span class="ident">find_ngrams</span></span>(<span>self, docs, threshold=-inf, verbose=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Perform n-gram replacement on the given documents using the phrase model
trained for this instance.</p>
<p>Detects significant bigrams by default, but bigrams with the same phrase
score are merged, and common terms are ignored (as described in the class
documentation).</p>
<p>To detect the next higher order of n-grams (viz., trigrams and above),
you will need to create a new ImprovedPhraser instance, passing the results
of this function as the new set of base sentences to use.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>docs</code></strong> :&ensp;<code>iterable</code> of <code>iterable</code> of <code>str</code></dt>
<dd>Each doc in docs should be an iterable of strings &ndash; Substring matching
will be used in the replacement process</dd>
<dt><strong><code>threshold</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Optionally set a new phrasing threshold; if not set, will apply all the
available phrases (which are determined by the value of <code>threshold</code>
passed to the base gensim model on init)</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>iterable</code> of <code>iterable</code> of <code>str</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def find_ngrams(self, docs, threshold=-math.inf, verbose=False):
    &#34;&#34;&#34;
    Perform n-gram replacement on the given documents using the phrase model
    trained for this instance.

    Detects significant bigrams by default, but bigrams with the same phrase
    score are merged, and common terms are ignored (as described in the class
    documentation).

    To detect the next higher order of n-grams (viz., trigrams and above),
    you will need to create a new ImprovedPhraser instance, passing the results
    of this function as the new set of base sentences to use.

    Parameters
    ----------
    docs: iterable of iterable of str
        Each doc in docs should be an iterable of strings -- Substring matching
        will be used in the replacement process
    threshold: float, optional
        Optionally set a new phrasing threshold; if not set, will apply all the
        available phrases (which are determined by the value of `threshold`
        passed to the base gensim model on init)
    verbose: bool, optional

    Returns
    -------
    iterable of iterable of str
    &#34;&#34;&#34;
    new_docs = []

    if verbose:
        docs = tqdm(docs)

    for doc in docs:
        # Find all candidate phrase chains originating from each token,
        # then merge the highest-scoring one.
        new_doc = doc[:]
        index = 0
        while index &lt; len(new_doc):
            token = new_doc[index]
            best_candidate = self._find_best_candidate(
                token, index, new_doc, threshold
            )

            # No phrases starting on this token
            if best_candidate is None:
                index += 1
                continue

            # Preemptively merge any candidates found in the chain starting from
            # this token
            exhausted_chain = False
            while best_candidate[&#34;start_index&#34;] &gt; index:
                # Merge
                new_doc[
                    best_candidate[&#34;start_index&#34;] : best_candidate[&#34;start_index&#34;]
                    + len(best_candidate[&#34;phrase&#34;])
                ] = [self.delimiter.join(best_candidate[&#34;phrase&#34;])]

                # Get the next highest scoring candidate in the chain
                best_candidate = self._find_best_candidate(
                    token, index, new_doc, threshold
                )

                if best_candidate is None:
                    # There is no candidate for merging anywhere in the chain,
                    # including directly from the current token...
                    exhausted_chain = True
                    break

            if exhausted_chain:
                # ... so continue with the next token in the document
                index += 1
                continue

            # Still here? Then there is a best candidate phrase for merging,
            # and it starts on this token
            assert best_candidate[&#34;start_index&#34;] == index
            new_doc[index : index + len(best_candidate[&#34;phrase&#34;])] = [
                self.delimiter.join(best_candidate[&#34;phrase&#34;])
            ]
            index += 1

        new_docs.append(new_doc)

    return new_docs</code></pre>
</details>
</dd>
<dt id="ignis.util.improved_phraser.ImprovedPhraser.find_ngrams_str"><code class="name flex">
<span>def <span class="ident">find_ngrams_str</span></span>(<span>self, docs, threshold=-inf)</span>
</code></dt>
<dd>
<div class="desc"><p>Perform n-gram replacement on the given documents using the phrase model
trained for this instance.</p>
<p>Version that uses string replacement: Much slower and slightly divergent from
the default algorithm, which always ensures that the right-most relevant
n-gram is merged first.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>docs</code></strong> :&ensp;<code>iterable</code> of <code>iterable</code> of <code>str</code></dt>
<dd>Each doc in docs should be an iterable of strings &ndash; Substring matching
will be used in the replacement process</dd>
<dt><strong><code>threshold</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Optionally set a new phrasing threshold; if not set, will apply all the
available phrases (which are determined by the value of <code>threshold</code>
passed to the base gensim model on init)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>iterable</code> of <code>iterable</code> of <code>str</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def find_ngrams_str(self, docs, threshold=-math.inf):
    &#34;&#34;&#34;
    Perform n-gram replacement on the given documents using the phrase model
    trained for this instance.

    Version that uses string replacement: Much slower and slightly divergent from
    the default algorithm, which always ensures that the right-most relevant
    n-gram is merged first.

    Parameters
    ----------
    docs: iterable of iterable of str
        Each doc in docs should be an iterable of strings -- Substring matching
        will be used in the replacement process
    threshold: float, optional
        Optionally set a new phrasing threshold; if not set, will apply all the
        available phrases (which are determined by the value of `threshold`
        passed to the base gensim model on init)

    Returns
    -------
    iterable of iterable of str
    &#34;&#34;&#34;
    # A &#34;safe&#34; delimiter that will be used for the intermediate string joins;
    # should be guaranteed to be different from `self.delimiter` so that we can
    # perform recursive phrasing without inadvertently re-splitting previously
    # joined tokens
    search_delimiter = f&#34;&lt;*&gt;&#34;

    new_docs = []
    for doc in docs:
        # In case we really need a new delimiter: Use part of a UUID (not the
        # whole, to conserve memory)
        while search_delimiter in doc or search_delimiter == self.delimiter:
            search_delimiter = f&#34;&lt;{str(uuid.uuid4())[:3]}&gt;&#34;

        str_doc = search_delimiter.join(doc)

        for phrase, score in self.phrases:
            if score &lt; threshold or phrase[0] not in doc:
                continue

            search_phrase = search_delimiter.join(phrase)
            replace_phrase = self.delimiter.join(phrase)

            str_phrase = f&#34;{search_delimiter}{search_phrase}{search_delimiter}&#34;
            str_replace = f&#34;{search_delimiter}{replace_phrase}{search_delimiter}&#34;
            str_doc = str_doc.replace(str_phrase, str_replace)

            # Handle matches at start/end of lines
            start_phrase = f&#34;{search_phrase}{search_delimiter}&#34;
            start_replace = f&#34;{replace_phrase}{search_delimiter}&#34;
            end_phrase = f&#34;{search_delimiter}{search_phrase}&#34;
            end_replace = f&#34;{search_delimiter}{replace_phrase}&#34;

            if str_doc.startswith(start_phrase):
                str_doc = start_replace + str_doc[len(start_phrase) :]
            if str_doc.endswith(end_phrase):
                str_doc = str_doc[0 : -len(end_phrase)] + end_replace

        new_docs.append(str_doc.split(search_delimiter))

    return new_docs</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div style="padding: 1em 0.5em">
<a href="/ignis-tm/ignis">
<div style="display: flex; align-items: center">
<img src="/ignis-tm/images/logo.png" alt="Ignis" height="50"/>
<span style="font-size: 2em; font-weight: 700; margin-left: 0.75em">Ignis</span>
</div>
</a>
</div>
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="ignis.util" href="index.html">ignis.util</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="ignis.util.improved_phraser.ImprovedPhraser" href="#ignis.util.improved_phraser.ImprovedPhraser">ImprovedPhraser</a></code></h4>
<ul class="">
<li><code><a title="ignis.util.improved_phraser.ImprovedPhraser.find_ngrams" href="#ignis.util.improved_phraser.ImprovedPhraser.find_ngrams">find_ngrams</a></code></li>
<li><code><a title="ignis.util.improved_phraser.ImprovedPhraser.find_ngrams_str" href="#ignis.util.improved_phraser.ImprovedPhraser.find_ngrams_str">find_ngrams_str</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>